{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>Reference: Guide to build Faster RCNN in PyTorch\n",
    "<a href=\"https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439\">https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439</a></p>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p>My Youtube: How FasterRCNN works and step-by-step PyTorch implementation <a href=\"https://youtu.be/4yOcsWg-7g8\">https://youtu.be/4yOcsWg-7g8</a></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda GeForce GTX 1660 SUPER\n"
     ]
    }
   ],
   "source": [
    "\n",
    "if(torch.cuda.is_available()):\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device, torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device= torch.device(\"cpu\")\n",
    "    print(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Read-a-batch-of-training-images-along-with-their-bounding-boxes-and-lables.\">Read a batch of training images along with their bounding boxes and lables.<a class=\"anchor-link\" href=\"#Read-a-batch-of-training-images-along-with-their-bounding-boxes-and-lables.\">¶</a></h1><p>In this example, I use read only 1 image, i.e., batch_size=1</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-1drr4hl0\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-94dde9ae6638>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# input image could be of any size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mimg0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'helmet 9.jpg'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mimg0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.5.1) C:\\Users\\appveyor\\AppData\\Local\\Temp\\1\\pip-req-build-1drr4hl0\\opencv\\modules\\imgproc\\src\\color.cpp:182: error: (-215:Assertion failed) !_src.empty() in function 'cv::cvtColor'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# input image could be of any size\n",
    "img0 = cv2.imread('helmet 9.jpg')\n",
    "img0 = cv2.cvtColor(img0, cv2.COLOR_BGR2RGB) \n",
    "print(img0.shape)\n",
    "plt.imshow(img0)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Object information: a set of bounding boxes [ymin, xmin, ymax, xmax] and their labels \n",
    "bbox0 = np.array([[160, 147, 260, 234], [139, 312, 200, 348]]) \n",
    "labels = np.array([1, 1]) # 0: background, 1: helmet\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display bounding box and labels\n",
    "# cv2.putText(影像, 文字, 座標, 字型, 大小, 顏色, 線條寬度, 線條種類)\n",
    "img0_clone = np.copy(img0)\n",
    "for i in range(len(bbox0)):\n",
    "    cv2.rectangle(img0_clone, (bbox0[i][1], bbox0[i][0]), (bbox0[i][3], bbox0[i][2]), color=(0, 255, 0), thickness=3) \n",
    "    cv2.putText(img0_clone, str(int(labels[i])), (bbox0[i][3], bbox0[i][2]), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255),thickness=3) \n",
    "plt.imshow(img0_clone)\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Resize-the-input-images-to-(h=800,-w=800)\">Resize the input images to (h=800, w=800)<a class=\"anchor-link\" href=\"#Resize-the-input-images-to-(h=800,-w=800)\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "img = cv2.resize(img0, dsize=(800, 800), interpolation=cv2.INTER_CUBIC)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# change the bounding box coordinates \n",
    "Wratio = 800/img0.shape[1]\n",
    "Hratio = 800/img0.shape[0]\n",
    "ratioLst = [Hratio, Wratio, Hratio, Wratio]\n",
    "bbox = []\n",
    "for box in bbox0:\n",
    "    box = [int(a * b) for a, b in zip(box, ratioLst)] \n",
    "    bbox.append(box)\n",
    "bbox = np.array(bbox)\n",
    "print(bbox)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display bounding box and labels\n",
    "img_clone = np.copy(img)\n",
    "bbox_clone = bbox.astype(int)\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3) # Draw Rectangle\n",
    "    cv2.putText(img_clone, str(int(labels[i])), (bbox[i][3], bbox[i][2]), cv2.FONT_HERSHEY_SIMPLEX, 3, (0,0,255),thickness=3) # Write the prediction class\n",
    "plt.imshow(img_clone)\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Use-VGG16-to-extract-features-from-input-images\">Use VGG16 to extract features from input images<a class=\"anchor-link\" href=\"#Use-VGG16-to-extract-features-from-input-images\">¶</a></h1><h2 id=\"Input-images-(batch_size,-H=800,-W=800,-d=3),--Features:-(batch_size,-H=-50,-W=50,-d=512)\">Input images (batch_size, H=800, W=800, d=3),  Features: (batch_size, H= 50, W=50, d=512)<a class=\"anchor-link\" href=\"#Input-images-(batch_size,-H=800,-W=800,-d=3),--Features:-(batch_size,-H=-50,-W=50,-d=512)\">¶</a></h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List all the layers of VGG16\n",
    "model = torchvision.models.vgg16(pretrained=True).to(device)\n",
    "fe = list(model.features)\n",
    "print(len(fe))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# collect layers with output feature map size (W, H) < 50\n",
    "dummy_img = torch.zeros((1, 3, 800, 800)).float() # test image array [1, 3, 800, 800] \n",
    "print(dummy_img.shape)\n",
    "\n",
    "req_features = []\n",
    "k = dummy_img.clone().to(device)\n",
    "for i in fe:\n",
    "    k = i(k)\n",
    "    if k.size()[2] < 800//16:   #800/16=50\n",
    "        break\n",
    "    req_features.append(i)\n",
    "    out_channels = k.size()[1]\n",
    "print(len(req_features)) #30\n",
    "print(out_channels) # 512\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert this list into a Sequential module\n",
    "faster_rcnn_fe_extractor = nn.Sequential(*req_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 測試看看 input image 通過 feature extractor 的結果\n",
    "transform = transforms.Compose([transforms.ToTensor()]) # Defing PyTorch Transform\n",
    "imgTensor = transform(img).to(device) \n",
    "imgTensor = imgTensor.unsqueeze(0)\n",
    "out_map = faster_rcnn_fe_extractor(imgTensor)\n",
    "print(out_map.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# visualize the first 5 channels of the 50*50*512 feature maps\n",
    "imgArray=out_map.data.cpu().numpy().squeeze(0)\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5): \n",
    "    fig.add_subplot(1, 5, figNo) \n",
    "    plt.imshow(imgArray[i], cmap='gray')\n",
    "    figNo +=1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"Generate-22,500-anchor-boxes-on-each-input-image\">Generate 22,500 anchor boxes on each input image<a class=\"anchor-link\" href=\"#Generate-22,500-anchor-boxes-on-each-input-image\">¶</a></h1><p>50x50=2500 anchors, each anchor generate 9 anchor boxes, Total = 50x50x9=22,500</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# x, y intervals to generate anchor box center\n",
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\n",
    "print(len(ctr_x), ctr_x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# coordinates of the 2500 center points to generate anchor boxes\n",
    "index = 0\n",
    "ctr = np.zeros((2500, 2))\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        ctr[index, 1] = ctr_x[x] - 8\n",
    "        ctr[index, 0] = ctr_y[y] - 8\n",
    "        index +=1\n",
    "print(ctr.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display the 2500 anchors\n",
    "img_clone = np.copy(img)\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in range(ctr.shape[0]):\n",
    "    cv2.circle(img_clone, (int(ctr[i][0]), int(ctr[i][1])), radius=1, color=(255, 0, 0), thickness=1) \n",
    "plt.imshow(img_clone)\n",
    "plt.show()    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for each of the 2500 anchors, generate 9 anchor boxes\n",
    "# 2500*9 = 22500 anchor boxes\n",
    "ratios = [0.5, 1, 2]\n",
    "scales = [8, 16, 32]\n",
    "sub_sample = 16\n",
    "anchor_boxes = np.zeros( ((fe_size * fe_size * 9), 4))\n",
    "index = 0\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):\n",
    "        for j in range(len(scales)):\n",
    "            h = sub_sample * scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample * scales[j] * np.sqrt(1./ ratios[i])\n",
    "            anchor_boxes[index, 0] = ctr_y - h / 2.\n",
    "            anchor_boxes[index, 1] = ctr_x - w / 2.\n",
    "            anchor_boxes[index, 2] = ctr_y + h / 2.\n",
    "            anchor_boxes[index, 3] = ctr_x + w / 2.\n",
    "            index += 1\n",
    "print(anchor_boxes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display the 9 anchor boxes of one anchor and the ground trugh bbox\n",
    "img_clone = np.copy(img)\n",
    "for i in range(11025, 11034):  #9*1225=11025\n",
    "    x0 = int(anchor_boxes[i][1])\n",
    "    y0 = int(anchor_boxes[i][0])\n",
    "    x1 = int(anchor_boxes[i][3])\n",
    "    y1 = int(anchor_boxes[i][2])\n",
    "    cv2.rectangle(img_clone, (x0, y0), (x1, y1), color=(255, 255, 2550), thickness=3) \n",
    "\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3) # Draw Rectangle\n",
    "    \n",
    "plt.imshow(img_clone)\n",
    "plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"計算每個-valid-anchor-boxe-與-ground-truth-bboxes-的-iou\">計算每個 valid anchor boxe 與 ground truth bboxes 的 iou<a class=\"anchor-link\" href=\"#計算每個-valid-anchor-boxe-與-ground-truth-bboxes-的-iou\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ignore cross-boundary anchor boxes\n",
    "# valid anchor boxes with (y1, x1)>0 and (y2, x2)<=800\n",
    "index_inside = np.where(\n",
    "        (anchor_boxes[:, 0] >= 0) &\n",
    "        (anchor_boxes[:, 1] >= 0) &\n",
    "        (anchor_boxes[:, 2] <= 800) &\n",
    "        (anchor_boxes[:, 3] <= 800)\n",
    "    )[0]\n",
    "print(index_inside.shape)\n",
    "\n",
    "valid_anchor_boxes = anchor_boxes[index_inside]\n",
    "print(valid_anchor_boxes.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Calculate iou of the valid anchor boxes \n",
    "# Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. \n",
    "ious = np.empty((len(valid_anchor_boxes), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = iter_area / (anchor_area+ box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What anchor box has max iou with the ground truth bbox  \n",
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)\n",
    "\n",
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# What ground truth bbox is associated with each anchor box \n",
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"依據-iou-值來設定-8940-個-valid-anchor-boxes-的標籤,-1:-object,-0:-background,--1:-ignore\">依據 iou 值來設定 8940 個 valid anchor boxes 的標籤, 1: object, 0: background, -1: ignore<a class=\"anchor-link\" href=\"#依據-iou-值來設定-8940-個-valid-anchor-boxes-的標籤,-1:-object,-0:-background,--1:-ignore\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 把 8940 個 valid anchor boxes 的標籤先統一設為 -1 (ignore)\n",
    "label = np.empty((len(index_inside), ), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Use iou to assign 1 (objects) to two kind of anchors \n",
    "# a) The anchors with the highest iou overlap with a ground-truth-box\n",
    "# b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box\n",
    "\n",
    "# Assign 0 (background) to an anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes\n",
    "pos_iou_threshold  = 0.7\n",
    "neg_iou_threshold = 0.3\n",
    "label[gt_argmax_ious] = 1\n",
    "label[max_ious >= pos_iou_threshold] = 1\n",
    "label[max_ious < neg_iou_threshold] = 0\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"每次-mini-batch-training-只取-256-個-valid-anchor-boxes-來訓練-RPN,-其中-128-positive-examples,-128-negative-examples-(標籤-0),-,-其他的-valid-anchor-boxes-的標籤在本次-mini-batch-training-改回--1-(ignore)\">每次 mini-batch training 只取 256 個 valid anchor boxes 來訓練 RPN, 其中 128 positive examples, 128 negative examples (標籤 0), , 其他的 valid anchor boxes 的標籤在本次 mini-batch training 改回 -1 (ignore)<a class=\"anchor-link\" href=\"#每次-mini-batch-training-只取-256-個-valid-anchor-boxes-來訓練-RPN,-其中-128-positive-examples,-128-negative-examples-(標籤-0),-,-其他的-valid-anchor-boxes-的標籤在本次-mini-batch-training-改回--1-(ignore)\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_sample = 256\n",
    "pos_ratio = 0.5\n",
    "n_pos = pos_ratio * n_sample\n",
    "\n",
    "pos_index = np.where(label == 1)[0]\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    label[disable_index] = -1\n",
    "    \n",
    "n_neg = n_sample * np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
    "    label[disable_index] = -1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"把-valid-anchor-boxes-的-format-從-(y1,-x,-y2,-x2)-轉成-loc-=-(cy-cya/ha),-(cx-cxa/wa),-log(h/ha),-log(w/wa),-以便訓練RPN-學習anchor-box-相對-ground-truth-box-的差異-(中心點差異,-box-H,-W-差異)\">把 valid anchor boxes 的 format 從 (y1, x, y2, x2) 轉成 loc = (cy-cya/ha), (cx-cxa/wa), log(h/ha), log(w/wa), 以便訓練RPN 學習anchor box 相對 ground truth box 的差異 (中心點差異, box H, W 差異)<a class=\"anchor-link\" href=\"#把-valid-anchor-boxes-的-format-從-(y1,-x,-y2,-x2)-轉成-loc-=-(cy-cya/ha),-(cx-cxa/wa),-log(h/ha),-log(w/wa),-以便訓練RPN-學習anchor-box-相對-ground-truth-box-的差異-(中心點差異,-box-H,-W-差異)\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For each valid anchor box, find the groundtruth object which has max_iou \n",
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox.shape)\n",
    "\n",
    "# valid anchor boxes 的 h, w, cx, cy \n",
    "height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
    "width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width\n",
    "\n",
    "# valid anchor box 的 max iou 的 bbox 的 h, w, cx, cy \n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width\n",
    "\n",
    "# valid anchor boxes 的 loc = (y-ya/ha), (x-xa/wa), log(h/ha), log(w/wa)\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps) #讓 height !=0, 最小值為 eps\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"先把-22500-個-anchor-boxes-的-label=-1-,-locations=0,-再填上-8940-個-valid-anchor-boxes-的-locations-與-labels\">先把 22500 個 anchor boxes 的 label=-1 , locations=0, 再填上 8940 個 valid anchor boxes 的 locations 與 labels<a class=\"anchor-link\" href=\"#先把-22500-個-anchor-boxes-的-label=-1-,-locations=0,-再填上-8940-個-valid-anchor-boxes-的-locations-與-labels\">¶</a></h1><p>NOTICE: 每個 training epoch, 我們從 8940 個 valid anchor boxes 隨機選 128 個 positive + 128個 negative, 其他都標-1</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "anchor_labels = np.empty((len(anchor_boxes),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label\n",
    "print(anchor_labels.shape)\n",
    "\n",
    "anchor_locations = np.empty((len(anchor_boxes),) + anchor_boxes.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs\n",
    "print(anchor_locations.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"把輸入影像的-features-送進-Region-Proposal-Network-(RPN),-預測-22500-個-region-proposals-(ROIs)\">把輸入影像的 features 送進 Region Proposal Network (RPN), 預測 22500 個 region proposals (ROIs)<a class=\"anchor-link\" href=\"#把輸入影像的-features-送進-Region-Proposal-Network-(RPN),-預測-22500-個-region-proposals-(ROIs)\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "mid_channels = 512\n",
    "n_anchor = 9  # Number of anchors at each location\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1).to(device)\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor *4, 1, 1, 0).to(device)\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor *2, 1, 1, 0).to(device) ## I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1.\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "x = conv1(out_map.to(device)) # out_map = faster_rcnn_fe_extractor(imgTensor)\n",
    "pred_anchor_locs = reg_layer(x)\n",
    "pred_cls_scores = cls_layer(x)\n",
    "print(pred_anchor_locs.shape, pred_cls_scores.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 轉換 RPN 預測 anchor box 的位置與分類之 format \n",
    "# 位置: [1, 36(9*4), 50, 50] => [1, 22500(50*50*9), 4] (dy, dx, dh, dw)\n",
    "# 分類: [1, 18(9*2), 50, 50] => [1, 22500, 2]  (1, 0)\n",
    "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(pred_anchor_locs.shape)\n",
    "\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "\n",
    "objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(objectness_score.shape)\n",
    "\n",
    "pred_cls_scores  = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"依據-RPN-預測的22500-個ROIs,-以及-22500-個-anchor-boxes,-計算-RPN-loss\">依據 RPN 預測的22500 個ROIs, 以及 22500 個 anchor boxes, 計算 RPN loss<a class=\"anchor-link\" href=\"#依據-RPN-預測的22500-個ROIs,-以及-22500-個-anchor-boxes,-計算-RPN-loss\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(pred_anchor_locs.shape)\n",
    "print(pred_cls_scores.shape)\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rpn_loc = pred_anchor_locs[0]\n",
    "rpn_score = pred_cls_scores[0]\n",
    "\n",
    "gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "\n",
    "print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For classification we use cross-entropy loss\n",
    "rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long().to(device), ignore_index = -1)\n",
    "print(rpn_cls_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# For Regression we use smooth L1 loss as defined in the Fast RCNN paper\n",
    "pos = gt_rpn_score > 0\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes which have positve labels\n",
    "mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\n",
    "print(rpn_loc_loss.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combining both the rpn_cls_loss and rpn_reg_loss\n",
    "rpn_lambda = 10.\n",
    "N_reg = (gt_rpn_score >0).float().sum()\n",
    "rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
    "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\n",
    "print(rpn_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"把-RPN-預測的-22500-個-ROIs-送-Fast-RCNN-預測-bbox-+-classifications\">把 RPN 預測的 22500 個 ROIs 送 Fast RCNN 預測 bbox + classifications<a class=\"anchor-link\" href=\"#把-RPN-預測的-22500-個-ROIs-送-Fast-RCNN-預測-bbox-+-classifications\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"先用-NMS-(Non-maximum-supression)-把-22500個-ROI-縮減到-2000-個\">先用 NMS (Non-maximum supression) 把 22500個 ROI 縮減到 2000 個<a class=\"anchor-link\" href=\"#先用-NMS-(Non-maximum-supression)-把-22500個-ROI-縮減到-2000-個\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "nms_thresh = 0.7  # non-maximum supression (NMS) \n",
    "n_train_pre_nms = 12000 # no. of train pre-NMS\n",
    "n_train_post_nms = 2000 # after nms, training Fast R-CNN using 2000 RPN proposals\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300 # During testing we evaluate 300 proposals,\n",
    "min_size = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# The labelled 22500 anchor boxes \n",
    "# format converted from [y1, x1, y2, x2] to [ctr_x, ctr_y, h, w]\n",
    "anc_height = anchor_boxes[:, 2] - anchor_boxes[:, 0]\n",
    "anc_width = anchor_boxes[:, 3] - anchor_boxes[:, 1]\n",
    "anc_ctr_y = anchor_boxes[:, 0] + 0.5 * anc_height\n",
    "anc_ctr_x = anchor_boxes[:, 1] + 0.5 * anc_width\n",
    "print(anc_ctr_x.shape)\n",
    "\n",
    "# The 22500 anchor boxes location and labels predicted by RPN (convert to numpy)\n",
    "# format = (dy, dx, dh, dw)\n",
    "pred_anchor_locs_numpy = pred_anchor_locs[0].cpu().data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].cpu().data.numpy()\n",
    "dy = pred_anchor_locs_numpy[:, 0::4] #每個 anchor box 的 dy\n",
    "dx = pred_anchor_locs_numpy[:, 1::4] # dx\n",
    "dh = pred_anchor_locs_numpy[:, 2::4] # dh\n",
    "dw = pred_anchor_locs_numpy[:, 3::4] # dw\n",
    "print(dy.shape)\n",
    "\n",
    "# ctr_y = dy predicted by RPN * anchor_h + anchor_cy\n",
    "# ctr_x similar\n",
    "# h = exp(dh predicted by RPN) * anchor_h\n",
    "# w similar\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]\n",
    "print(w.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 用 labelled 的 anchor boxes 與 RPN 預測的 anchor boxes來計算 ROI = [y1, x1, y2, x2] \n",
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=anchor_locs.dtype)\n",
    "roi[:, 0::4] = ctr_y - 0.5 * h\n",
    "roi[:, 1::4] = ctr_x - 0.5 * w\n",
    "roi[:, 2::4] = ctr_y + 0.5 * h\n",
    "roi[:, 3::4] = ctr_x + 0.5 * w\n",
    "print(roi.shape)\n",
    "\n",
    "# clip the predicted boxes to the image\n",
    "img_size = (800, 800) #Image size\n",
    "roi[:, slice(0, 4, 2)] = np.clip(roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "roi[:, slice(1, 4, 2)] = np.clip(roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "print(roi.shape, np.max(roi), np.min(roi))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove predicted boxes with either height or width < threshold.\n",
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0] #min_size=16\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "print(keep.shape, roi.shape, score.shape)\n",
    "\n",
    "# Sort all (proposal, score) pairs by score from highest to lowest\n",
    "order = score.ravel().argsort()[::-1]\n",
    "print(order.shape)\n",
    "\n",
    "#Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "order = order[:n_train_pre_nms]\n",
    "roi = roi[order, :]\n",
    "print(order.shape, roi.shape, roi.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Take all the roi boxes [roi_array]\n",
    "y1 = roi[:, 0]\n",
    "x1 = roi[:, 1]\n",
    "y2 = roi[:, 2]\n",
    "x2 = roi[:, 3]\n",
    "\n",
    "# Find the areas of all the boxes [roi_area]\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Take the indexes of order the probability score in descending order \n",
    "order = order.argsort()[::-1]\n",
    "keep = []\n",
    "while (order.size > 0):\n",
    "    i = order[0] #take the 1st elt in order and append to keep \n",
    "    keep.append(i)\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]]) \n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "    inter = w * h\n",
    "    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "keep = keep[:n_train_post_nms] # while training/testing , use accordingly\n",
    "roi = roi[keep] # the final region proposals\n",
    "print(len(keep), roi.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"針對NMS-篩出的-2000-個-ROIs,-計算與-gt-box-之-iou-值,-篩選出-128-個-ROI-samples,-其中-positive-最多-128x0.25=32個\">針對NMS 篩出的 2000 個 ROIs, 計算與 gt box 之 iou 值, 篩選出 128 個 ROI samples, 其中 positive 最多 128x0.25=32個<a class=\"anchor-link\" href=\"#針對NMS-篩出的-2000-個-ROIs,-計算與-gt-box-之-iou-值,-篩選出-128-個-ROI-samples,-其中-positive-最多-128x0.25=32個\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "n_sample = 128  # Number of samples from roi \n",
    "pos_ratio = 0.25 # Number of positive examples out of the n_samples\n",
    "pos_iou_thresh = 0.5  # Min iou of region proposal with any groundtruth object to consider it as positive label\n",
    "neg_iou_thresh_hi = 0.5  # iou 0~0.5 is considered as negative (0, background)\n",
    "neg_iou_thresh_lo = 0.0\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find the iou of each ground truth object with the region proposals, \n",
    "ious = np.empty((len(roi), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "for num1, i in enumerate(roi):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * (inter_x2 - inter_x1)\n",
    "            iou = iter_area / (anchor_area+ box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Find out which ground truth has high IoU for each region proposal, Also find the maximum IoU\n",
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_iou = ious.max(axis=1)\n",
    "print(gt_assignment)\n",
    "print(max_iou)\n",
    "\n",
    "# Assign the labels to each proposal\n",
    "gt_roi_label = labels[gt_assignment]\n",
    "print(gt_roi_label)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Select the foreground rois as per the pos_iou_thesh and \n",
    "# n_sample x pos_ratio (128 x 0.25 = 32) foreground samples.\n",
    "pos_roi_per_image = 32 \n",
    "pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(\n",
    "        pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)\n",
    "\n",
    "# Similarly we do for negitive (background) region proposals\n",
    "neg_index = np.where((max_iou < neg_iou_thresh_hi) &\n",
    "                             (max_iou >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image, neg_index.size))\n",
    "if  neg_index.size > 0 :\n",
    "    neg_index = np.random.choice(\n",
    "        neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display ROI samples with postive \n",
    "img_clone = np.copy(img)\n",
    "for i in range(pos_roi_per_this_image):\n",
    "    y0, x0, y1, x1 = roi[pos_index[i]].astype(int)\n",
    "    cv2.rectangle(img_clone, (x0, y0), (x1, y1), color=(255, 255, 2550), thickness=3) \n",
    "\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3) # Draw Rectangle\n",
    "    \n",
    "plt.imshow(img_clone)\n",
    "plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# display ROI samples with negative \n",
    "img_clone = np.copy(img)\n",
    "plt.figure(figsize=(9, 6))\n",
    "for i in range(neg_roi_per_this_image):\n",
    "    y0, x0, y1, x1 = roi[neg_index[i]].astype(int)\n",
    "    cv2.rectangle(img_clone, (x0, y0), (x1, y1), color=(255, 255, 2550), thickness=3) \n",
    "\n",
    "for i in range(len(bbox)):\n",
    "    cv2.rectangle(img_clone, (bbox[i][1], bbox[i][0]), (bbox[i][3], bbox[i][2]), color=(0, 255, 0), thickness=3) # Draw Rectangle\n",
    "    \n",
    "plt.imshow(img_clone)\n",
    "plt.show()  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Now we gather positve samples index and negitive samples index, \n",
    "# their respective labels and region proposals\n",
    "\n",
    "keep_index = np.append(pos_index, neg_index)\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)\n",
    "\n",
    "# Pick the ground truth objects for these sample_roi and \n",
    "# later parameterize as we have done while assigning locations to anchor boxes in section 2.\n",
    "bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)\n",
    "\n",
    "height = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "width = sample_roi[:, 3] - sample_roi[:, 1]\n",
    "ctr_y = sample_roi[:, 0] + 0.5 * height\n",
    "ctr_x = sample_roi[:, 1] + 0.5 * width\n",
    "\n",
    "base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "\n",
    "gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(gt_roi_locs.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"取出128-個-ROI-samples-的-features,-用-max-pooling-調成-same-size,-H=7,-W=7-(ROI-Pooling)\">取出128 個 ROI samples 的 features, 用 max pooling 調成 same size, H=7, W=7 (ROI Pooling)<a class=\"anchor-link\" href=\"#取出128-個-ROI-samples-的-features,-用-max-pooling-調成-same-size,-H=7,-W=7-(ROI-Pooling)\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "rois = torch.from_numpy(sample_roi).float()\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "print(rois.shape, roi_indices.shape)\n",
    "\n",
    "indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "print(xy_indices_and_rois.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output = []\n",
    "rois = indices_and_rois.data.float()\n",
    "rois[:, 1:].mul_(1/16.0) # Subsampling ratio\n",
    "rois = rois.long()\n",
    "num_rois = rois.size(0)\n",
    "for i in range(num_rois):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = adaptive_max_pool(im)\n",
    "    output.append(tmp[0])\n",
    "output = torch.cat(output, 0)\n",
    "print(output.size())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the first 5 ROI's feature map (for each feature map, only show the 1st channel of d=512)\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = im[0][0].detach().cpu().numpy()\n",
    "    fig.add_subplot(1, 5, figNo) \n",
    "    plt.imshow(tmp, cmap='gray')\n",
    "    figNo +=1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Visualize the first 5 ROI's feature maps after ROI pooling (for each feature map, only show the 1st channel of d=512)\n",
    "fig=plt.figure(figsize=(12, 4))\n",
    "figNo = 1\n",
    "for i in range(5):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = adaptive_max_pool(im)[0]\n",
    "    tmp = tmp[0][0].detach().cpu().numpy()\n",
    "    fig.add_subplot(1, 5, figNo) \n",
    "    plt.imshow(tmp, cmap='gray')\n",
    "    figNo +=1\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "k = output.view(output.size(0), -1)\n",
    "print(k.shape) # 25088 = 7*7*512\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"128-個-ROI-samples-的-boxes-+-features-(7x7x512)-送到-Detection-network-預測輸入影像的物件-bounding-box-與-class\">128 個 ROI samples 的 boxes + features (7x7x512) 送到 Detection network 預測輸入影像的物件 bounding box 與 class<a class=\"anchor-link\" href=\"#128-個-ROI-samples-的-boxes-+-features-(7x7x512)-送到-Detection-network-預測輸入影像的物件-bounding-box-與-class\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096), nn.Linear(4096, 4096)]).to(device)\n",
    "cls_loc = nn.Linear(4096, 2 * 4).to(device) # (1 classes 安全帽 + 1 background. Each will have 4 co-ordinates)\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "\n",
    "score = nn.Linear(4096, 2).to(device) # (1 classes, 安全帽 + 1 background)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# passing the output of roi-pooling to ROI head \n",
    "k = roi_head_classifier(k.to(device))\n",
    "roi_cls_loc = cls_loc(k)\n",
    "roi_cls_score = score(k)\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<h1 id=\"依據這-128-個ROI-對應的-gt-bboxes-以及-features-(h,-w,-d=512),--計算-Fast-RCNN-的-loss\">依據這 128 個ROI 對應的 gt bboxes 以及 features (h, w, d=512),  計算 Fast RCNN 的 loss<a class=\"anchor-link\" href=\"#依據這-128-個ROI-對應的-gt-bboxes-以及-features-(h,-w,-d=512),--計算-Fast-RCNN-的-loss\">¶</a></h1>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# predicted\n",
    "print(roi_cls_loc.shape)\n",
    "print(roi_cls_score.shape)\n",
    "\n",
    "#actual\n",
    "print(gt_roi_locs.shape)\n",
    "print(gt_roi_labels.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gt_roi_labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Converting ground truth to torch variable\n",
    "gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "print(gt_roi_loc.shape, gt_roi_label.shape)\n",
    "\n",
    "#Classification loss\n",
    "roi_cls_loss = F.cross_entropy(roi_cls_score.cpu(), gt_roi_label.cpu(), ignore_index=-1)\n",
    "print(roi_cls_loss.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Regression loss\n",
    "n_sample = roi_cls_loc.shape[0]\n",
    "roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "print(roi_loc.shape)\n",
    "\n",
    "roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_loc.shape)\n",
    "\n",
    "# For Regression we use smooth L1 loss as defined in the Fast RCNN paper\n",
    "pos = gt_roi_label > 0\n",
    "mask = pos.unsqueeze(1).expand_as(roi_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes which have positve labels\n",
    "mask_loc_preds = roi_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_roi_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "roi_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\n",
    "print(roi_loc_loss.sum())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "roi_lambda = 10.\n",
    "roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)\n",
    "print(roi_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_loss = rpn_loss + roi_loss\n",
    "print(total_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    " \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
