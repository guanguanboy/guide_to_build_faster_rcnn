{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://medium.com/@fractaldle/guide-to-build-faster-rcnn-in-pytorch-95b10c273439\n",
    "内容来自以上连接\n",
    "\n",
    "另一个参考连接：https://medium.com/@fractaldle/brief-overview-on-object-detection-algorithms-ec516929be93"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin with an image and a set of bounding boxes along with its label as defined below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "iamge = torch.zeros((1, 3, 800, 800)).float()\n",
    "bbox = torch.FloatTensor([[20, 30, 400, 500], [300, 400, 500, 600]])\n",
    "# [y1, x1, y2, x2] format\n",
    "labels = torch.LongTensor([6, 8]) # 0 represents background\n",
    "sub_sample = 16\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Create a dummy image and set the volatile to be False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 800, 800])\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "dummy_img = torch.zeros((1, 3, 800, 800)).float()\n",
    "print(dummy_img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. List all the layers of the VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)), ReLU(inplace=True), MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)]\n"
     ]
    }
   ],
   "source": [
    "model = torchvision.models.vgg16(pretrained=True)\n",
    "fe = list(model.features)\n",
    "print(fe)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Pass the image through the layers and check where you are getting this size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "512\n"
     ]
    }
   ],
   "source": [
    "req_features = []\n",
    "k = dummy_img.clone()\n",
    "for i in fe:\n",
    "    k = i(k)\n",
    "    if k.size()[2] < 800//16:\n",
    "        break\n",
    "    req_features.append(i)\n",
    "    out_channels = k.size()[1]\n",
    "    \n",
    "print(len(req_features))\n",
    "print(out_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. Convert this list into a Sequential module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "faster_rcnn_fe_extractor = nn.Sequential(*req_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this faster_rcnn_fe_extractor can be used as our backend. Lets compute the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 512, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "out_map = faster_rcnn_fe_extractor(dummy_img)\n",
    "print(out_map.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor boxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first encounter with anchor boxes. A detailed understanding of anchor boxes will allow us to understand object detection very easily. So lets talk in detail on how this is done.\n",
    "\n",
    "1,Generate Anchor at a feature map location\n",
    "2,Generate Anchor at all the feature map location.\n",
    "3,Assign the labels and location of objects (with respect to the anchor) to each and every anchor.\n",
    "4,Generate Anchor at a feature map location\n",
    "\n",
    "We will use anchor_scales of 8, 16, 32, ratio of 0.5, 1, 2 and sub sampling of 16 (Since we have pooled our image from 800 px to 50px). Now every pixel in the output feature map maps to corresponding 16 * 16 pixels in the image.\n",
    "\n",
    "We need to generate anchor boxes on top of this 16 * 16 pixels first and similarly do along x-axis and y-axis to get all the anchor boxes. This is done in the step-2.\n",
    "\n",
    "At each pixel location on the feature map, We need to generate 9 anchor boxes (number of anchor_scales and number of ratios) and each anchor box will have ‘y1’, ‘x1’, ‘y2’, ‘x2’. So at each location anchor will have a shape of (9, 4). Lets begin with a an empty array filled with zero values.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "ratios = [0.5, 1, 2]\n",
    "anchor_scales = [8, 16, 32]\n",
    "anchor_base = np.zeros((len(ratios) * len(anchor_scales), 4), dtype=np.float32)\n",
    "\n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets fill these values with corresponding y1, x1, y2, x2 at each anchor_scale and ratios. Our center for this base anchor will be at"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.0 8.0\n",
      "[[ -37.254833  -82.50967    53.254833   98.50967 ]\n",
      " [ -82.50967  -173.01933    98.50967   189.01933 ]\n",
      " [-173.01933  -354.03867   189.01933   370.03867 ]\n",
      " [ -56.        -56.         72.         72.      ]\n",
      " [-120.       -120.        136.        136.      ]\n",
      " [-248.       -248.        264.        264.      ]\n",
      " [ -82.50967   -37.254833   98.50967    53.254833]\n",
      " [-173.01933   -82.50967   189.01933    98.50967 ]\n",
      " [-354.03867  -173.01933   370.03867   189.01933 ]]\n"
     ]
    }
   ],
   "source": [
    "ctr_y = sub_sample / 2\n",
    "ctr_x = sub_sample / 2\n",
    "\n",
    "print(ctr_y, ctr_x)\n",
    "\n",
    "for i in range(len(ratios)):\n",
    "    for j in range(len(anchor_scales)):\n",
    "        h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "        w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "        \n",
    "        index = i * len(anchor_scales) + j\n",
    "        \n",
    "        anchor_base[index, 0] = ctr_y - h / 2\n",
    "        anchor_base[index, 1] = ctr_x - w / 2\n",
    "        anchor_base[index, 2] = ctr_y + h / 2\n",
    "        anchor_base[index, 3] = ctr_x + w / 2\n",
    "        \n",
    "        \n",
    "print(anchor_base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the anchor locations at the first feature map pixel, we have to now generate these anchors at all the locations of feature map. Also note that negitive values mean that the anchor boxes are outside image dimension. In the later section we will label them with -1 and remove them when calculating the loss the functions and generating proposals for anchor boxes. Also Since we got 9 anchors at each location and there 50 * 50 such locations inside an image, We will get 17500 (50 * 50 * 9) anchors in total. Lets generate other anchors now,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Generate Anchor at all the feature map location.\n",
    "In-order to do this, we need to first generate the centres for each and every feature map pixel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "fe_size = (800//16)\n",
    "ctr_x = np.arange(16, (fe_size+1) * 16, 16)\n",
    "ctr_y = np.arange(16, (fe_size+1) * 16, 16)\n",
    "\n",
    "print(len(ctr_x))\n",
    "print(len(ctr_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looping through the ctr_x and ctr_y will give us the centers at each and every location. The sudo code is as a below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500\n",
      "[(8, 8), (24, 8), (40, 8), (56, 8), (72, 8), (88, 8), (104, 8), (120, 8), (136, 8), (152, 8), (168, 8), (184, 8), (200, 8), (216, 8), (232, 8), (248, 8), (264, 8), (280, 8), (296, 8), (312, 8), (328, 8), (344, 8), (360, 8), (376, 8), (392, 8), (408, 8), (424, 8), (440, 8), (456, 8), (472, 8), (488, 8), (504, 8), (520, 8), (536, 8), (552, 8), (568, 8), (584, 8), (600, 8), (616, 8), (632, 8), (648, 8), (664, 8), (680, 8), (696, 8), (712, 8), (728, 8), (744, 8), (760, 8), (776, 8), (792, 8), (8, 24), (24, 24), (40, 24), (56, 24), (72, 24), (88, 24), (104, 24), (120, 24), (136, 24), (152, 24), (168, 24), (184, 24), (200, 24), (216, 24), (232, 24), (248, 24), (264, 24), (280, 24), (296, 24), (312, 24), (328, 24), (344, 24), (360, 24), (376, 24), (392, 24), (408, 24), (424, 24), (440, 24), (456, 24), (472, 24), (488, 24), (504, 24), (520, 24), (536, 24), (552, 24), (568, 24), (584, 24), (600, 24), (616, 24), (632, 24), (648, 24), (664, 24), (680, 24), (696, 24), (712, 24), (728, 24), (744, 24), (760, 24), (776, 24), (792, 24), (8, 40), (24, 40), (40, 40), (56, 40), (72, 40), (88, 40), (104, 40), (120, 40), (136, 40), (152, 40), (168, 40), (184, 40), (200, 40), (216, 40), (232, 40), (248, 40), (264, 40), (280, 40), (296, 40), (312, 40), (328, 40), (344, 40), (360, 40), (376, 40), (392, 40), (408, 40), (424, 40), (440, 40), (456, 40), (472, 40), (488, 40), (504, 40), (520, 40), (536, 40), (552, 40), (568, 40), (584, 40), (600, 40), (616, 40), (632, 40), (648, 40), (664, 40), (680, 40), (696, 40), (712, 40), (728, 40), (744, 40), (760, 40), (776, 40), (792, 40), (8, 56), (24, 56), (40, 56), (56, 56), (72, 56), (88, 56), (104, 56), (120, 56), (136, 56), (152, 56), (168, 56), (184, 56), (200, 56), (216, 56), (232, 56), (248, 56), (264, 56), (280, 56), (296, 56), (312, 56), (328, 56), (344, 56), (360, 56), (376, 56), (392, 56), (408, 56), (424, 56), (440, 56), (456, 56), (472, 56), (488, 56), (504, 56), (520, 56), (536, 56), (552, 56), (568, 56), (584, 56), (600, 56), (616, 56), (632, 56), (648, 56), (664, 56), (680, 56), (696, 56), (712, 56), (728, 56), (744, 56), (760, 56), (776, 56), (792, 56), (8, 72), (24, 72), (40, 72), (56, 72), (72, 72), (88, 72), (104, 72), (120, 72), (136, 72), (152, 72), (168, 72), (184, 72), (200, 72), (216, 72), (232, 72), (248, 72), (264, 72), (280, 72), (296, 72), (312, 72), (328, 72), (344, 72), (360, 72), (376, 72), (392, 72), (408, 72), (424, 72), (440, 72), (456, 72), (472, 72), (488, 72), (504, 72), (520, 72), (536, 72), (552, 72), (568, 72), (584, 72), (600, 72), (616, 72), (632, 72), (648, 72), (664, 72), (680, 72), (696, 72), (712, 72), (728, 72), (744, 72), (760, 72), (776, 72), (792, 72), (8, 88), (24, 88), (40, 88), (56, 88), (72, 88), (88, 88), (104, 88), (120, 88), (136, 88), (152, 88), (168, 88), (184, 88), (200, 88), (216, 88), (232, 88), (248, 88), (264, 88), (280, 88), (296, 88), (312, 88), (328, 88), (344, 88), (360, 88), (376, 88), (392, 88), (408, 88), (424, 88), (440, 88), (456, 88), (472, 88), (488, 88), (504, 88), (520, 88), (536, 88), (552, 88), (568, 88), (584, 88), (600, 88), (616, 88), (632, 88), (648, 88), (664, 88), (680, 88), (696, 88), (712, 88), (728, 88), (744, 88), (760, 88), (776, 88), (792, 88), (8, 104), (24, 104), (40, 104), (56, 104), (72, 104), (88, 104), (104, 104), (120, 104), (136, 104), (152, 104), (168, 104), (184, 104), (200, 104), (216, 104), (232, 104), (248, 104), (264, 104), (280, 104), (296, 104), (312, 104), (328, 104), (344, 104), (360, 104), (376, 104), (392, 104), (408, 104), (424, 104), (440, 104), (456, 104), (472, 104), (488, 104), (504, 104), (520, 104), (536, 104), (552, 104), (568, 104), (584, 104), (600, 104), (616, 104), (632, 104), (648, 104), (664, 104), (680, 104), (696, 104), (712, 104), (728, 104), (744, 104), (760, 104), (776, 104), (792, 104), (8, 120), (24, 120), (40, 120), (56, 120), (72, 120), (88, 120), (104, 120), (120, 120), (136, 120), (152, 120), (168, 120), (184, 120), (200, 120), (216, 120), (232, 120), (248, 120), (264, 120), (280, 120), (296, 120), (312, 120), (328, 120), (344, 120), (360, 120), (376, 120), (392, 120), (408, 120), (424, 120), (440, 120), (456, 120), (472, 120), (488, 120), (504, 120), (520, 120), (536, 120), (552, 120), (568, 120), (584, 120), (600, 120), (616, 120), (632, 120), (648, 120), (664, 120), (680, 120), (696, 120), (712, 120), (728, 120), (744, 120), (760, 120), (776, 120), (792, 120), (8, 136), (24, 136), (40, 136), (56, 136), (72, 136), (88, 136), (104, 136), (120, 136), (136, 136), (152, 136), (168, 136), (184, 136), (200, 136), (216, 136), (232, 136), (248, 136), (264, 136), (280, 136), (296, 136), (312, 136), (328, 136), (344, 136), (360, 136), (376, 136), (392, 136), (408, 136), (424, 136), (440, 136), (456, 136), (472, 136), (488, 136), (504, 136), (520, 136), (536, 136), (552, 136), (568, 136), (584, 136), (600, 136), (616, 136), (632, 136), (648, 136), (664, 136), (680, 136), (696, 136), (712, 136), (728, 136), (744, 136), (760, 136), (776, 136), (792, 136), (8, 152), (24, 152), (40, 152), (56, 152), (72, 152), (88, 152), (104, 152), (120, 152), (136, 152), (152, 152), (168, 152), (184, 152), (200, 152), (216, 152), (232, 152), (248, 152), (264, 152), (280, 152), (296, 152), (312, 152), (328, 152), (344, 152), (360, 152), (376, 152), (392, 152), (408, 152), (424, 152), (440, 152), (456, 152), (472, 152), (488, 152), (504, 152), (520, 152), (536, 152), (552, 152), (568, 152), (584, 152), (600, 152), (616, 152), (632, 152), (648, 152), (664, 152), (680, 152), (696, 152), (712, 152), (728, 152), (744, 152), (760, 152), (776, 152), (792, 152), (8, 168), (24, 168), (40, 168), (56, 168), (72, 168), (88, 168), (104, 168), (120, 168), (136, 168), (152, 168), (168, 168), (184, 168), (200, 168), (216, 168), (232, 168), (248, 168), (264, 168), (280, 168), (296, 168), (312, 168), (328, 168), (344, 168), (360, 168), (376, 168), (392, 168), (408, 168), (424, 168), (440, 168), (456, 168), (472, 168), (488, 168), (504, 168), (520, 168), (536, 168), (552, 168), (568, 168), (584, 168), (600, 168), (616, 168), (632, 168), (648, 168), (664, 168), (680, 168), (696, 168), (712, 168), (728, 168), (744, 168), (760, 168), (776, 168), (792, 168), (8, 184), (24, 184), (40, 184), (56, 184), (72, 184), (88, 184), (104, 184), (120, 184), (136, 184), (152, 184), (168, 184), (184, 184), (200, 184), (216, 184), (232, 184), (248, 184), (264, 184), (280, 184), (296, 184), (312, 184), (328, 184), (344, 184), (360, 184), (376, 184), (392, 184), (408, 184), (424, 184), (440, 184), (456, 184), (472, 184), (488, 184), (504, 184), (520, 184), (536, 184), (552, 184), (568, 184), (584, 184), (600, 184), (616, 184), (632, 184), (648, 184), (664, 184), (680, 184), (696, 184), (712, 184), (728, 184), (744, 184), (760, 184), (776, 184), (792, 184), (8, 200), (24, 200), (40, 200), (56, 200), (72, 200), (88, 200), (104, 200), (120, 200), (136, 200), (152, 200), (168, 200), (184, 200), (200, 200), (216, 200), (232, 200), (248, 200), (264, 200), (280, 200), (296, 200), (312, 200), (328, 200), (344, 200), (360, 200), (376, 200), (392, 200), (408, 200), (424, 200), (440, 200), (456, 200), (472, 200), (488, 200), (504, 200), (520, 200), (536, 200), (552, 200), (568, 200), (584, 200), (600, 200), (616, 200), (632, 200), (648, 200), (664, 200), (680, 200), (696, 200), (712, 200), (728, 200), (744, 200), (760, 200), (776, 200), (792, 200), (8, 216), (24, 216), (40, 216), (56, 216), (72, 216), (88, 216), (104, 216), (120, 216), (136, 216), (152, 216), (168, 216), (184, 216), (200, 216), (216, 216), (232, 216), (248, 216), (264, 216), (280, 216), (296, 216), (312, 216), (328, 216), (344, 216), (360, 216), (376, 216), (392, 216), (408, 216), (424, 216), (440, 216), (456, 216), (472, 216), (488, 216), (504, 216), (520, 216), (536, 216), (552, 216), (568, 216), (584, 216), (600, 216), (616, 216), (632, 216), (648, 216), (664, 216), (680, 216), (696, 216), (712, 216), (728, 216), (744, 216), (760, 216), (776, 216), (792, 216), (8, 232), (24, 232), (40, 232), (56, 232), (72, 232), (88, 232), (104, 232), (120, 232), (136, 232), (152, 232), (168, 232), (184, 232), (200, 232), (216, 232), (232, 232), (248, 232), (264, 232), (280, 232), (296, 232), (312, 232), (328, 232), (344, 232), (360, 232), (376, 232), (392, 232), (408, 232), (424, 232), (440, 232), (456, 232), (472, 232), (488, 232), (504, 232), (520, 232), (536, 232), (552, 232), (568, 232), (584, 232), (600, 232), (616, 232), (632, 232), (648, 232), (664, 232), (680, 232), (696, 232), (712, 232), (728, 232), (744, 232), (760, 232), (776, 232), (792, 232), (8, 248), (24, 248), (40, 248), (56, 248), (72, 248), (88, 248), (104, 248), (120, 248), (136, 248), (152, 248), (168, 248), (184, 248), (200, 248), (216, 248), (232, 248), (248, 248), (264, 248), (280, 248), (296, 248), (312, 248), (328, 248), (344, 248), (360, 248), (376, 248), (392, 248), (408, 248), (424, 248), (440, 248), (456, 248), (472, 248), (488, 248), (504, 248), (520, 248), (536, 248), (552, 248), (568, 248), (584, 248), (600, 248), (616, 248), (632, 248), (648, 248), (664, 248), (680, 248), (696, 248), (712, 248), (728, 248), (744, 248), (760, 248), (776, 248), (792, 248), (8, 264), (24, 264), (40, 264), (56, 264), (72, 264), (88, 264), (104, 264), (120, 264), (136, 264), (152, 264), (168, 264), (184, 264), (200, 264), (216, 264), (232, 264), (248, 264), (264, 264), (280, 264), (296, 264), (312, 264), (328, 264), (344, 264), (360, 264), (376, 264), (392, 264), (408, 264), (424, 264), (440, 264), (456, 264), (472, 264), (488, 264), (504, 264), (520, 264), (536, 264), (552, 264), (568, 264), (584, 264), (600, 264), (616, 264), (632, 264), (648, 264), (664, 264), (680, 264), (696, 264), (712, 264), (728, 264), (744, 264), (760, 264), (776, 264), (792, 264), (8, 280), (24, 280), (40, 280), (56, 280), (72, 280), (88, 280), (104, 280), (120, 280), (136, 280), (152, 280), (168, 280), (184, 280), (200, 280), (216, 280), (232, 280), (248, 280), (264, 280), (280, 280), (296, 280), (312, 280), (328, 280), (344, 280), (360, 280), (376, 280), (392, 280), (408, 280), (424, 280), (440, 280), (456, 280), (472, 280), (488, 280), (504, 280), (520, 280), (536, 280), (552, 280), (568, 280), (584, 280), (600, 280), (616, 280), (632, 280), (648, 280), (664, 280), (680, 280), (696, 280), (712, 280), (728, 280), (744, 280), (760, 280), (776, 280), (792, 280), (8, 296), (24, 296), (40, 296), (56, 296), (72, 296), (88, 296), (104, 296), (120, 296), (136, 296), (152, 296), (168, 296), (184, 296), (200, 296), (216, 296), (232, 296), (248, 296), (264, 296), (280, 296), (296, 296), (312, 296), (328, 296), (344, 296), (360, 296), (376, 296), (392, 296), (408, 296), (424, 296), (440, 296), (456, 296), (472, 296), (488, 296), (504, 296), (520, 296), (536, 296), (552, 296), (568, 296), (584, 296), (600, 296), (616, 296), (632, 296), (648, 296), (664, 296), (680, 296), (696, 296), (712, 296), (728, 296), (744, 296), (760, 296), (776, 296), (792, 296), (8, 312), (24, 312), (40, 312), (56, 312), (72, 312), (88, 312), (104, 312), (120, 312), (136, 312), (152, 312), (168, 312), (184, 312), (200, 312), (216, 312), (232, 312), (248, 312), (264, 312), (280, 312), (296, 312), (312, 312), (328, 312), (344, 312), (360, 312), (376, 312), (392, 312), (408, 312), (424, 312), (440, 312), (456, 312), (472, 312), (488, 312), (504, 312), (520, 312), (536, 312), (552, 312), (568, 312), (584, 312), (600, 312), (616, 312), (632, 312), (648, 312), (664, 312), (680, 312), (696, 312), (712, 312), (728, 312), (744, 312), (760, 312), (776, 312), (792, 312), (8, 328), (24, 328), (40, 328), (56, 328), (72, 328), (88, 328), (104, 328), (120, 328), (136, 328), (152, 328), (168, 328), (184, 328), (200, 328), (216, 328), (232, 328), (248, 328), (264, 328), (280, 328), (296, 328), (312, 328), (328, 328), (344, 328), (360, 328), (376, 328), (392, 328), (408, 328), (424, 328), (440, 328), (456, 328), (472, 328), (488, 328), (504, 328), (520, 328), (536, 328), (552, 328), (568, 328), (584, 328), (600, 328), (616, 328), (632, 328), (648, 328), (664, 328), (680, 328), (696, 328), (712, 328), (728, 328), (744, 328), (760, 328), (776, 328), (792, 328), (8, 344), (24, 344), (40, 344), (56, 344), (72, 344), (88, 344), (104, 344), (120, 344), (136, 344), (152, 344), (168, 344), (184, 344), (200, 344), (216, 344), (232, 344), (248, 344), (264, 344), (280, 344), (296, 344), (312, 344), (328, 344), (344, 344), (360, 344), (376, 344), (392, 344), (408, 344), (424, 344), (440, 344), (456, 344), (472, 344), (488, 344), (504, 344), (520, 344), (536, 344), (552, 344), (568, 344), (584, 344), (600, 344), (616, 344), (632, 344), (648, 344), (664, 344), (680, 344), (696, 344), (712, 344), (728, 344), (744, 344), (760, 344), (776, 344), (792, 344), (8, 360), (24, 360), (40, 360), (56, 360), (72, 360), (88, 360), (104, 360), (120, 360), (136, 360), (152, 360), (168, 360), (184, 360), (200, 360), (216, 360), (232, 360), (248, 360), (264, 360), (280, 360), (296, 360), (312, 360), (328, 360), (344, 360), (360, 360), (376, 360), (392, 360), (408, 360), (424, 360), (440, 360), (456, 360), (472, 360), (488, 360), (504, 360), (520, 360), (536, 360), (552, 360), (568, 360), (584, 360), (600, 360), (616, 360), (632, 360), (648, 360), (664, 360), (680, 360), (696, 360), (712, 360), (728, 360), (744, 360), (760, 360), (776, 360), (792, 360), (8, 376), (24, 376), (40, 376), (56, 376), (72, 376), (88, 376), (104, 376), (120, 376), (136, 376), (152, 376), (168, 376), (184, 376), (200, 376), (216, 376), (232, 376), (248, 376), (264, 376), (280, 376), (296, 376), (312, 376), (328, 376), (344, 376), (360, 376), (376, 376), (392, 376), (408, 376), (424, 376), (440, 376), (456, 376), (472, 376), (488, 376), (504, 376), (520, 376), (536, 376), (552, 376), (568, 376), (584, 376), (600, 376), (616, 376), (632, 376), (648, 376), (664, 376), (680, 376), (696, 376), (712, 376), (728, 376), (744, 376), (760, 376), (776, 376), (792, 376), (8, 392), (24, 392), (40, 392), (56, 392), (72, 392), (88, 392), (104, 392), (120, 392), (136, 392), (152, 392), (168, 392), (184, 392), (200, 392), (216, 392), (232, 392), (248, 392), (264, 392), (280, 392), (296, 392), (312, 392), (328, 392), (344, 392), (360, 392), (376, 392), (392, 392), (408, 392), (424, 392), (440, 392), (456, 392), (472, 392), (488, 392), (504, 392), (520, 392), (536, 392), (552, 392), (568, 392), (584, 392), (600, 392), (616, 392), (632, 392), (648, 392), (664, 392), (680, 392), (696, 392), (712, 392), (728, 392), (744, 392), (760, 392), (776, 392), (792, 392), (8, 408), (24, 408), (40, 408), (56, 408), (72, 408), (88, 408), (104, 408), (120, 408), (136, 408), (152, 408), (168, 408), (184, 408), (200, 408), (216, 408), (232, 408), (248, 408), (264, 408), (280, 408), (296, 408), (312, 408), (328, 408), (344, 408), (360, 408), (376, 408), (392, 408), (408, 408), (424, 408), (440, 408), (456, 408), (472, 408), (488, 408), (504, 408), (520, 408), (536, 408), (552, 408), (568, 408), (584, 408), (600, 408), (616, 408), (632, 408), (648, 408), (664, 408), (680, 408), (696, 408), (712, 408), (728, 408), (744, 408), (760, 408), (776, 408), (792, 408), (8, 424), (24, 424), (40, 424), (56, 424), (72, 424), (88, 424), (104, 424), (120, 424), (136, 424), (152, 424), (168, 424), (184, 424), (200, 424), (216, 424), (232, 424), (248, 424), (264, 424), (280, 424), (296, 424), (312, 424), (328, 424), (344, 424), (360, 424), (376, 424), (392, 424), (408, 424), (424, 424), (440, 424), (456, 424), (472, 424), (488, 424), (504, 424), (520, 424), (536, 424), (552, 424), (568, 424), (584, 424), (600, 424), (616, 424), (632, 424), (648, 424), (664, 424), (680, 424), (696, 424), (712, 424), (728, 424), (744, 424), (760, 424), (776, 424), (792, 424), (8, 440), (24, 440), (40, 440), (56, 440), (72, 440), (88, 440), (104, 440), (120, 440), (136, 440), (152, 440), (168, 440), (184, 440), (200, 440), (216, 440), (232, 440), (248, 440), (264, 440), (280, 440), (296, 440), (312, 440), (328, 440), (344, 440), (360, 440), (376, 440), (392, 440), (408, 440), (424, 440), (440, 440), (456, 440), (472, 440), (488, 440), (504, 440), (520, 440), (536, 440), (552, 440), (568, 440), (584, 440), (600, 440), (616, 440), (632, 440), (648, 440), (664, 440), (680, 440), (696, 440), (712, 440), (728, 440), (744, 440), (760, 440), (776, 440), (792, 440), (8, 456), (24, 456), (40, 456), (56, 456), (72, 456), (88, 456), (104, 456), (120, 456), (136, 456), (152, 456), (168, 456), (184, 456), (200, 456), (216, 456), (232, 456), (248, 456), (264, 456), (280, 456), (296, 456), (312, 456), (328, 456), (344, 456), (360, 456), (376, 456), (392, 456), (408, 456), (424, 456), (440, 456), (456, 456), (472, 456), (488, 456), (504, 456), (520, 456), (536, 456), (552, 456), (568, 456), (584, 456), (600, 456), (616, 456), (632, 456), (648, 456), (664, 456), (680, 456), (696, 456), (712, 456), (728, 456), (744, 456), (760, 456), (776, 456), (792, 456), (8, 472), (24, 472), (40, 472), (56, 472), (72, 472), (88, 472), (104, 472), (120, 472), (136, 472), (152, 472), (168, 472), (184, 472), (200, 472), (216, 472), (232, 472), (248, 472), (264, 472), (280, 472), (296, 472), (312, 472), (328, 472), (344, 472), (360, 472), (376, 472), (392, 472), (408, 472), (424, 472), (440, 472), (456, 472), (472, 472), (488, 472), (504, 472), (520, 472), (536, 472), (552, 472), (568, 472), (584, 472), (600, 472), (616, 472), (632, 472), (648, 472), (664, 472), (680, 472), (696, 472), (712, 472), (728, 472), (744, 472), (760, 472), (776, 472), (792, 472), (8, 488), (24, 488), (40, 488), (56, 488), (72, 488), (88, 488), (104, 488), (120, 488), (136, 488), (152, 488), (168, 488), (184, 488), (200, 488), (216, 488), (232, 488), (248, 488), (264, 488), (280, 488), (296, 488), (312, 488), (328, 488), (344, 488), (360, 488), (376, 488), (392, 488), (408, 488), (424, 488), (440, 488), (456, 488), (472, 488), (488, 488), (504, 488), (520, 488), (536, 488), (552, 488), (568, 488), (584, 488), (600, 488), (616, 488), (632, 488), (648, 488), (664, 488), (680, 488), (696, 488), (712, 488), (728, 488), (744, 488), (760, 488), (776, 488), (792, 488), (8, 504), (24, 504), (40, 504), (56, 504), (72, 504), (88, 504), (104, 504), (120, 504), (136, 504), (152, 504), (168, 504), (184, 504), (200, 504), (216, 504), (232, 504), (248, 504), (264, 504), (280, 504), (296, 504), (312, 504), (328, 504), (344, 504), (360, 504), (376, 504), (392, 504), (408, 504), (424, 504), (440, 504), (456, 504), (472, 504), (488, 504), (504, 504), (520, 504), (536, 504), (552, 504), (568, 504), (584, 504), (600, 504), (616, 504), (632, 504), (648, 504), (664, 504), (680, 504), (696, 504), (712, 504), (728, 504), (744, 504), (760, 504), (776, 504), (792, 504), (8, 520), (24, 520), (40, 520), (56, 520), (72, 520), (88, 520), (104, 520), (120, 520), (136, 520), (152, 520), (168, 520), (184, 520), (200, 520), (216, 520), (232, 520), (248, 520), (264, 520), (280, 520), (296, 520), (312, 520), (328, 520), (344, 520), (360, 520), (376, 520), (392, 520), (408, 520), (424, 520), (440, 520), (456, 520), (472, 520), (488, 520), (504, 520), (520, 520), (536, 520), (552, 520), (568, 520), (584, 520), (600, 520), (616, 520), (632, 520), (648, 520), (664, 520), (680, 520), (696, 520), (712, 520), (728, 520), (744, 520), (760, 520), (776, 520), (792, 520), (8, 536), (24, 536), (40, 536), (56, 536), (72, 536), (88, 536), (104, 536), (120, 536), (136, 536), (152, 536), (168, 536), (184, 536), (200, 536), (216, 536), (232, 536), (248, 536), (264, 536), (280, 536), (296, 536), (312, 536), (328, 536), (344, 536), (360, 536), (376, 536), (392, 536), (408, 536), (424, 536), (440, 536), (456, 536), (472, 536), (488, 536), (504, 536), (520, 536), (536, 536), (552, 536), (568, 536), (584, 536), (600, 536), (616, 536), (632, 536), (648, 536), (664, 536), (680, 536), (696, 536), (712, 536), (728, 536), (744, 536), (760, 536), (776, 536), (792, 536), (8, 552), (24, 552), (40, 552), (56, 552), (72, 552), (88, 552), (104, 552), (120, 552), (136, 552), (152, 552), (168, 552), (184, 552), (200, 552), (216, 552), (232, 552), (248, 552), (264, 552), (280, 552), (296, 552), (312, 552), (328, 552), (344, 552), (360, 552), (376, 552), (392, 552), (408, 552), (424, 552), (440, 552), (456, 552), (472, 552), (488, 552), (504, 552), (520, 552), (536, 552), (552, 552), (568, 552), (584, 552), (600, 552), (616, 552), (632, 552), (648, 552), (664, 552), (680, 552), (696, 552), (712, 552), (728, 552), (744, 552), (760, 552), (776, 552), (792, 552), (8, 568), (24, 568), (40, 568), (56, 568), (72, 568), (88, 568), (104, 568), (120, 568), (136, 568), (152, 568), (168, 568), (184, 568), (200, 568), (216, 568), (232, 568), (248, 568), (264, 568), (280, 568), (296, 568), (312, 568), (328, 568), (344, 568), (360, 568), (376, 568), (392, 568), (408, 568), (424, 568), (440, 568), (456, 568), (472, 568), (488, 568), (504, 568), (520, 568), (536, 568), (552, 568), (568, 568), (584, 568), (600, 568), (616, 568), (632, 568), (648, 568), (664, 568), (680, 568), (696, 568), (712, 568), (728, 568), (744, 568), (760, 568), (776, 568), (792, 568), (8, 584), (24, 584), (40, 584), (56, 584), (72, 584), (88, 584), (104, 584), (120, 584), (136, 584), (152, 584), (168, 584), (184, 584), (200, 584), (216, 584), (232, 584), (248, 584), (264, 584), (280, 584), (296, 584), (312, 584), (328, 584), (344, 584), (360, 584), (376, 584), (392, 584), (408, 584), (424, 584), (440, 584), (456, 584), (472, 584), (488, 584), (504, 584), (520, 584), (536, 584), (552, 584), (568, 584), (584, 584), (600, 584), (616, 584), (632, 584), (648, 584), (664, 584), (680, 584), (696, 584), (712, 584), (728, 584), (744, 584), (760, 584), (776, 584), (792, 584), (8, 600), (24, 600), (40, 600), (56, 600), (72, 600), (88, 600), (104, 600), (120, 600), (136, 600), (152, 600), (168, 600), (184, 600), (200, 600), (216, 600), (232, 600), (248, 600), (264, 600), (280, 600), (296, 600), (312, 600), (328, 600), (344, 600), (360, 600), (376, 600), (392, 600), (408, 600), (424, 600), (440, 600), (456, 600), (472, 600), (488, 600), (504, 600), (520, 600), (536, 600), (552, 600), (568, 600), (584, 600), (600, 600), (616, 600), (632, 600), (648, 600), (664, 600), (680, 600), (696, 600), (712, 600), (728, 600), (744, 600), (760, 600), (776, 600), (792, 600), (8, 616), (24, 616), (40, 616), (56, 616), (72, 616), (88, 616), (104, 616), (120, 616), (136, 616), (152, 616), (168, 616), (184, 616), (200, 616), (216, 616), (232, 616), (248, 616), (264, 616), (280, 616), (296, 616), (312, 616), (328, 616), (344, 616), (360, 616), (376, 616), (392, 616), (408, 616), (424, 616), (440, 616), (456, 616), (472, 616), (488, 616), (504, 616), (520, 616), (536, 616), (552, 616), (568, 616), (584, 616), (600, 616), (616, 616), (632, 616), (648, 616), (664, 616), (680, 616), (696, 616), (712, 616), (728, 616), (744, 616), (760, 616), (776, 616), (792, 616), (8, 632), (24, 632), (40, 632), (56, 632), (72, 632), (88, 632), (104, 632), (120, 632), (136, 632), (152, 632), (168, 632), (184, 632), (200, 632), (216, 632), (232, 632), (248, 632), (264, 632), (280, 632), (296, 632), (312, 632), (328, 632), (344, 632), (360, 632), (376, 632), (392, 632), (408, 632), (424, 632), (440, 632), (456, 632), (472, 632), (488, 632), (504, 632), (520, 632), (536, 632), (552, 632), (568, 632), (584, 632), (600, 632), (616, 632), (632, 632), (648, 632), (664, 632), (680, 632), (696, 632), (712, 632), (728, 632), (744, 632), (760, 632), (776, 632), (792, 632), (8, 648), (24, 648), (40, 648), (56, 648), (72, 648), (88, 648), (104, 648), (120, 648), (136, 648), (152, 648), (168, 648), (184, 648), (200, 648), (216, 648), (232, 648), (248, 648), (264, 648), (280, 648), (296, 648), (312, 648), (328, 648), (344, 648), (360, 648), (376, 648), (392, 648), (408, 648), (424, 648), (440, 648), (456, 648), (472, 648), (488, 648), (504, 648), (520, 648), (536, 648), (552, 648), (568, 648), (584, 648), (600, 648), (616, 648), (632, 648), (648, 648), (664, 648), (680, 648), (696, 648), (712, 648), (728, 648), (744, 648), (760, 648), (776, 648), (792, 648), (8, 664), (24, 664), (40, 664), (56, 664), (72, 664), (88, 664), (104, 664), (120, 664), (136, 664), (152, 664), (168, 664), (184, 664), (200, 664), (216, 664), (232, 664), (248, 664), (264, 664), (280, 664), (296, 664), (312, 664), (328, 664), (344, 664), (360, 664), (376, 664), (392, 664), (408, 664), (424, 664), (440, 664), (456, 664), (472, 664), (488, 664), (504, 664), (520, 664), (536, 664), (552, 664), (568, 664), (584, 664), (600, 664), (616, 664), (632, 664), (648, 664), (664, 664), (680, 664), (696, 664), (712, 664), (728, 664), (744, 664), (760, 664), (776, 664), (792, 664), (8, 680), (24, 680), (40, 680), (56, 680), (72, 680), (88, 680), (104, 680), (120, 680), (136, 680), (152, 680), (168, 680), (184, 680), (200, 680), (216, 680), (232, 680), (248, 680), (264, 680), (280, 680), (296, 680), (312, 680), (328, 680), (344, 680), (360, 680), (376, 680), (392, 680), (408, 680), (424, 680), (440, 680), (456, 680), (472, 680), (488, 680), (504, 680), (520, 680), (536, 680), (552, 680), (568, 680), (584, 680), (600, 680), (616, 680), (632, 680), (648, 680), (664, 680), (680, 680), (696, 680), (712, 680), (728, 680), (744, 680), (760, 680), (776, 680), (792, 680), (8, 696), (24, 696), (40, 696), (56, 696), (72, 696), (88, 696), (104, 696), (120, 696), (136, 696), (152, 696), (168, 696), (184, 696), (200, 696), (216, 696), (232, 696), (248, 696), (264, 696), (280, 696), (296, 696), (312, 696), (328, 696), (344, 696), (360, 696), (376, 696), (392, 696), (408, 696), (424, 696), (440, 696), (456, 696), (472, 696), (488, 696), (504, 696), (520, 696), (536, 696), (552, 696), (568, 696), (584, 696), (600, 696), (616, 696), (632, 696), (648, 696), (664, 696), (680, 696), (696, 696), (712, 696), (728, 696), (744, 696), (760, 696), (776, 696), (792, 696), (8, 712), (24, 712), (40, 712), (56, 712), (72, 712), (88, 712), (104, 712), (120, 712), (136, 712), (152, 712), (168, 712), (184, 712), (200, 712), (216, 712), (232, 712), (248, 712), (264, 712), (280, 712), (296, 712), (312, 712), (328, 712), (344, 712), (360, 712), (376, 712), (392, 712), (408, 712), (424, 712), (440, 712), (456, 712), (472, 712), (488, 712), (504, 712), (520, 712), (536, 712), (552, 712), (568, 712), (584, 712), (600, 712), (616, 712), (632, 712), (648, 712), (664, 712), (680, 712), (696, 712), (712, 712), (728, 712), (744, 712), (760, 712), (776, 712), (792, 712), (8, 728), (24, 728), (40, 728), (56, 728), (72, 728), (88, 728), (104, 728), (120, 728), (136, 728), (152, 728), (168, 728), (184, 728), (200, 728), (216, 728), (232, 728), (248, 728), (264, 728), (280, 728), (296, 728), (312, 728), (328, 728), (344, 728), (360, 728), (376, 728), (392, 728), (408, 728), (424, 728), (440, 728), (456, 728), (472, 728), (488, 728), (504, 728), (520, 728), (536, 728), (552, 728), (568, 728), (584, 728), (600, 728), (616, 728), (632, 728), (648, 728), (664, 728), (680, 728), (696, 728), (712, 728), (728, 728), (744, 728), (760, 728), (776, 728), (792, 728), (8, 744), (24, 744), (40, 744), (56, 744), (72, 744), (88, 744), (104, 744), (120, 744), (136, 744), (152, 744), (168, 744), (184, 744), (200, 744), (216, 744), (232, 744), (248, 744), (264, 744), (280, 744), (296, 744), (312, 744), (328, 744), (344, 744), (360, 744), (376, 744), (392, 744), (408, 744), (424, 744), (440, 744), (456, 744), (472, 744), (488, 744), (504, 744), (520, 744), (536, 744), (552, 744), (568, 744), (584, 744), (600, 744), (616, 744), (632, 744), (648, 744), (664, 744), (680, 744), (696, 744), (712, 744), (728, 744), (744, 744), (760, 744), (776, 744), (792, 744), (8, 760), (24, 760), (40, 760), (56, 760), (72, 760), (88, 760), (104, 760), (120, 760), (136, 760), (152, 760), (168, 760), (184, 760), (200, 760), (216, 760), (232, 760), (248, 760), (264, 760), (280, 760), (296, 760), (312, 760), (328, 760), (344, 760), (360, 760), (376, 760), (392, 760), (408, 760), (424, 760), (440, 760), (456, 760), (472, 760), (488, 760), (504, 760), (520, 760), (536, 760), (552, 760), (568, 760), (584, 760), (600, 760), (616, 760), (632, 760), (648, 760), (664, 760), (680, 760), (696, 760), (712, 760), (728, 760), (744, 760), (760, 760), (776, 760), (792, 760), (8, 776), (24, 776), (40, 776), (56, 776), (72, 776), (88, 776), (104, 776), (120, 776), (136, 776), (152, 776), (168, 776), (184, 776), (200, 776), (216, 776), (232, 776), (248, 776), (264, 776), (280, 776), (296, 776), (312, 776), (328, 776), (344, 776), (360, 776), (376, 776), (392, 776), (408, 776), (424, 776), (440, 776), (456, 776), (472, 776), (488, 776), (504, 776), (520, 776), (536, 776), (552, 776), (568, 776), (584, 776), (600, 776), (616, 776), (632, 776), (648, 776), (664, 776), (680, 776), (696, 776), (712, 776), (728, 776), (744, 776), (760, 776), (776, 776), (792, 776), (8, 792), (24, 792), (40, 792), (56, 792), (72, 792), (88, 792), (104, 792), (120, 792), (136, 792), (152, 792), (168, 792), (184, 792), (200, 792), (216, 792), (232, 792), (248, 792), (264, 792), (280, 792), (296, 792), (312, 792), (328, 792), (344, 792), (360, 792), (376, 792), (392, 792), (408, 792), (424, 792), (440, 792), (456, 792), (472, 792), (488, 792), (504, 792), (520, 792), (536, 792), (552, 792), (568, 792), (584, 792), (600, 792), (616, 792), (632, 792), (648, 792), (664, 792), (680, 792), (696, 792), (712, 792), (728, 792), (744, 792), (760, 792), (776, 792), (792, 792)]\n"
     ]
    }
   ],
   "source": [
    "index = 0\n",
    "#ctr = list((len(ctr_x), len(ctr_y)))\n",
    "#print(size(ctr))\n",
    "#ctr = np.zeros((len(ctr_x), len(ctr_y)))\n",
    "ctr = []\n",
    "for x in range(len(ctr_x)):\n",
    "    for y in range(len(ctr_y)):\n",
    "        #ctr[index,1] = ctr_x[x] - 8\n",
    "        #ctr[index,0] = ctr_y[y] - 8\n",
    "        ctr.append((ctr_y[y] - 8, ctr_x[x] - 8))\n",
    "        index +=1\n",
    "        \n",
    "print(len(ctr))\n",
    "print(ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output will be the (x, y) value at each location as shown in the image above. Together we have 2500 anchor centers. Now at each center we need to generate the anchor boxes. This can be done using the code we have used for generating anchor at one location, adding an extract for loop for supplying centers of each anchor will do. Lets see how this is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchors = np.zeros((fe_size * fe_size * 9, 4), dtype=np.float32)\n",
    "index = 0\n",
    "for c in ctr:\n",
    "    ctr_y, ctr_x = c\n",
    "    for i in range(len(ratios)):\n",
    "        for j in range(len(anchor_scales)):\n",
    "            h = sub_sample * anchor_scales[j] * np.sqrt(ratios[i])\n",
    "            w = sub_sample * anchor_scales[j] * np.sqrt(1./ ratios[i])\n",
    "      \n",
    "            anchors[index, 0] = ctr_y - h / 2\n",
    "            anchors[index, 1] = ctr_x - w / 2\n",
    "            anchors[index, 2] = ctr_y + h / 2\n",
    "            anchors[index, 3] = ctr_x + w / 2\n",
    "            index += 1\n",
    "print(anchors.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign the labels and location of objects (with respect to the anchor) to each and every anchor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now since we have generated all the anchor boxes, we need to look at the objects inside the image and assign them to the specific anchor boxes which contain them. Faster_R-CNN has some guidelines to assign labels to the anchor boxes\n",
    "We assign a positive label to two kind of anchors a) The anchor/anchors with the highest Intersection-over-Union(IoU) overlap with a ground-truth-box or b) An anchor that has an IoU overlap higher than 0.7 with ground-truth box.\n",
    "Note that single ground-truth object may assign positive labels to multiple anchors.\n",
    "c) We assign a negative label to a non-positive anchor if its IoU ratio is lower than 0.3 for all ground-truth boxes. d) Anchors that are neither positive nor negitive do not contribute to the training objective.\n",
    "Lets see how this is done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 8]\n"
     ]
    }
   ],
   "source": [
    "bbox = np.asarray([[20, 30, 400, 500], [300, 400, 500, 600]], dtype=np.float32) # [y1, x1, y2, x2] format\n",
    "labels = np.asarray([6, 8], dtype=np.int8) # 0 represents background\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will assign the labels and locations for the anchor boxes in the following ways.\n",
    "Find the indexes of valid anchor boxes and create an array with these indexes. create an label array with shape index array filled with -1.\n",
    "check weather one of the above conditition a, b, c is statisfying or not and fill the label accordingly. Incase of positive anchor box (label is 1), Note which ground truth object has resulted in this.\n",
    "calculate the locations (loc) of ground truth associated with the anchor box wrt to the anchor box.\n",
    "Reorganize all anchor boxes by filling with -1 for all unvalid anchor boxes and values we have calculated for all valid anchor boxes.\n",
    "Outputs should be labels with (N, 1) array and locs with (N, 4) array.\n",
    "\n",
    "Find the index of all valid anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n"
     ]
    }
   ],
   "source": [
    "index_inside = np.where(\n",
    "        (anchors[:, 0] >= 0) &\n",
    "        (anchors[:, 1] >= 0) &\n",
    "        (anchors[:, 2] <= 800) &\n",
    "        (anchors[:, 3] <= 800)\n",
    "    )[0]\n",
    "print(index_inside.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an empty label array with inside_index shape and fill with -1. Default is set to (d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[-1 -1 -1 ... -1 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "label = np.empty((len(index_inside),), dtype=np.int32)\n",
    "label.fill(-1)\n",
    "print(label.shape)\n",
    "#print(label)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "create an array with valid anchor boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940, 4)\n"
     ]
    }
   ],
   "source": [
    "valid_anchor_boxes = anchors[index_inside]\n",
    "print(valid_anchor_boxes.shape)\n",
    "#Out = (8940, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each valid anchor box calculate the iou with each ground truth object. \n",
    "Since we have 8940 anchor boxes and 2 ground truth objects, we should get an array with (8490, 2) as the output. The sudo code for calculating iou between two boxes will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The python code for calculating the ious is as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  30. 400. 500.]\n",
      " [300. 400. 500. 600.]]\n",
      "(8940, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(valid_anchor_boxes), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "print(bbox)\n",
    "for num1, i in enumerate(valid_anchor_boxes):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * \\\n",
    "(inter_x2 - inter_x1)\n",
    "            iou = iter_area / \\\n",
    "(anchor_area+ box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: Using numpy arrays, these calculations can be done much more efficiently and with less verbose. However I try to keep here in this way so that people without strong Algebra can also understand.\n",
    "Considering the scenarios of a and b, we need to find two things here\n",
    "\n",
    "-- the highest iou for each gt_box and its corresponding anchor box\n",
    "-- the highest iou for each anchor box and its corresponding ground truth box"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 5620]\n",
      "[0.68130493 0.61035156]\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = ious.argmax(axis=0)\n",
    "print(gt_argmax_ious)\n",
    "\n",
    "gt_max_ious = ious[gt_argmax_ious, np.arange(ious.shape[1])]\n",
    "print(gt_max_ious)\n",
    "print(np.arange(ious.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "case-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8940,)\n",
      "[0 0 0 ... 0 0 0]\n",
      "[0.06811669 0.07083762 0.07083762 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "argmax_ious = ious.argmax(axis=1)\n",
    "print(argmax_ious.shape)\n",
    "print(argmax_ious)\n",
    "max_ious = ious[np.arange(len(index_inside)), argmax_ious]\n",
    "print(max_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find the anchor_boxes which have this max_ious (gt_max_ious)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2262 2508 5620 5628 5636 5644 5866 5874 5882 5890 6112 6120 6128 6136\n",
      " 6358 6366 6374 6382]\n"
     ]
    }
   ],
   "source": [
    "gt_argmax_ious = np.where(ious == gt_max_ious)[0]\n",
    "print(gt_argmax_ious)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have three arrays\n",
    "argmax_ious — Tells which ground truth object has max iou with each anchor.\n",
    "max_ious — Tells the max_iou with ground truth object with each anchor.\n",
    "gt_argmax_ious — Tells the anchors with the highest Intersection-over-Union (IoU) overlap with a ground-truth box.\n",
    "\n",
    "Using argmax_ious and max_ious we can assign labels and locations to anchor boxes which satisify [b] and [c]. Using gt_argmax_ious we can assign labels and locations to anchor boxes which satisify [a].\n",
    "\n",
    "Lets put thresholds to some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_iou_threshold  = 0.7\n",
    "neg_iou_threshold = 0.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign negitive label (0) to all the anchor boxes which have max_iou less than negitive threshold [c]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[max_ious < neg_iou_threshold] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign positive label (1) to all the anchor boxes which have highest IoU overlap with a ground-truth box [a]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[gt_argmax_ious] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign positive label (1) to all the anchor boxes which have max_iou greater than positive threshold [b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "label[max_ious >= pos_iou_threshold] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training RPN The Faster_R-CNN paper phrases as follows Each mini-batch arises from a single image that contains many positive and negitive example anchors, but this will bias towards negitive samples as they are dominate. Instead, we randomly sample 256 anchors in an image to compute the loss function of a mini-batch, where the sampled positive and negative anchors have a ratio of up to 1:1. If there are fewer than 128 positive samples in an image, we pad the mini-batch with negitive ones.. From this we can derive two variable as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_ratio = 0.5\n",
    "n_sample = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pos = pos_ratio * n_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to randomly sample n_pos samples from the positive labels and ignore (-1) the remaining ones. In some cases we get less than n_pos samples, in that we will randomly sample (n_sample — n_pos) negitive samples (0) and assign ignore label to the remaining anchor boxes. This is done using the following code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "positive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_index = np.where(label == 1)[0]\n",
    "if len(pos_index) > n_pos:\n",
    "    disable_index = np.random.choice(pos_index, size=(len(pos_index) - n_pos), replace=False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "negitive samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neg = n_sample - np.sum(label == 1)\n",
    "neg_index = np.where(label == 0)[0]\n",
    "if len(neg_index) > n_neg:\n",
    "    disable_index = np.random.choice(neg_index, size=(len(neg_index) - n_neg), replace = False)\n",
    "    label[disable_index] = -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assigning locations to anchor boxes\n",
    "Now lets assign the locations to each anchor box with the ground truth object which has maximum iou. Note, we will assign anchor locs to all the valid anchor boxes irrespective of its label, later when we are calculating the losses, we can remove them with simple filters.\n",
    "\n",
    "We already know which ground truth object has high iou with each anchor box, Now we need to find the locations of ground truth with respect to the anchor box location. Faster_R-CNN uses the following parametrizion for this\n",
    "\n",
    "t_{x} = (x - x_{a})/w_{a}\n",
    "t_{y} = (y - y_{a})/h_{a}\n",
    "t_{w} = log(w/ w_a)\n",
    "t_{h} = log(h/ h_a)\n",
    "\n",
    "x, y , w, h are the groud truth box center co-ordinates, width and height. x_a, y_a, h_a and w_a and anchor boxes center cooridinates, width and height."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each anchor box, find the groundtruth object which has max_iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " ...\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]\n",
      " [ 20.  30. 400. 500.]]\n"
     ]
    }
   ],
   "source": [
    "max_iou_bbox = bbox[argmax_ious]\n",
    "print(max_iou_bbox)\n",
    "#Out\n",
    "# [[ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  ...,\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.],\n",
    "#  [ 20.,  30., 400., 500.]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to find t_{x}, t_{y}, t_{w}, t_{h}, we need to convert the y1, x1, y2, x2 format of valid anchor boxes and associated ground truth boxes with max iou to ctr_y, ctr_x , h, w format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = valid_anchor_boxes[:, 2] - valid_anchor_boxes[:, 0]\n",
    "width = valid_anchor_boxes[:, 3] - valid_anchor_boxes[:, 1]\n",
    "ctr_y = valid_anchor_boxes[:, 0] + 0.5 * height\n",
    "ctr_x = valid_anchor_boxes[:, 1] + 0.5 * width\n",
    "base_height = max_iou_bbox[:, 2] - max_iou_bbox[:, 0]\n",
    "base_width = max_iou_bbox[:, 3] - max_iou_bbox[:, 1]\n",
    "base_ctr_y = max_iou_bbox[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = max_iou_bbox[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the above formulas to find the loc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5855727   2.3091455   0.7415673   1.647276  ]\n",
      " [ 0.49718437  2.3091455   0.7415673   1.647276  ]\n",
      " [ 0.40879607  2.3091455   0.7415673   1.647276  ]\n",
      " ...\n",
      " [-2.50802    -5.292254    0.7415677   1.6472763 ]\n",
      " [-2.5964084  -5.292254    0.7415677   1.6472763 ]\n",
      " [-2.6847968  -5.292254    0.7415677   1.6472763 ]]\n"
     ]
    }
   ],
   "source": [
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "anchor_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(anchor_locs)\n",
    "#Out:\n",
    "# [[ 0.5855727   2.3091455   0.7415673   1.647276  ]\n",
    "#  [ 0.49718437  2.3091455   0.7415673   1.647276  ]\n",
    "#  [ 0.40879607  2.3091455   0.7415673   1.647276  ]\n",
    "#  ...\n",
    "#  [-2.50802    -5.292254    0.7415677   1.6472763 ]\n",
    "#  [-2.5964084  -5.292254    0.7415677   1.6472763 ]\n",
    "#  [-2.6847968  -5.292254    0.7415677   1.6472763 ]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we got anchor_locs and label associated with each and every valid anchor boxes\n",
    "Lets map them to the original anchors using the inside_index variable. Fill the unvalid anchor boxes labels with -1 (ignore) and locations with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "(22500, 4)\n"
     ]
    }
   ],
   "source": [
    "anchor_labels = np.empty((len(anchors),), dtype=label.dtype)\n",
    "anchor_labels.fill(-1)\n",
    "anchor_labels[index_inside] = label\n",
    "\n",
    "print(anchor_labels.shape)\n",
    "\n",
    "anchor_locations = np.empty((len(anchors),) + anchors.shape[1:], dtype=anchor_locs.dtype)\n",
    "anchor_locations.fill(0)\n",
    "anchor_locations[index_inside, :] = anchor_locs\n",
    "\n",
    "print(anchor_locations.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final two matrices are\n",
    "anchor_locations [N, 4] — [22500, 4]\n",
    "anchor_labels [N,] — [22500]\n",
    "These are used as targets to the RPN network. We will see how this RPN network is designed in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Region Proposal Network.\n",
    "\n",
    "As we have discussed earlier, Prior to this work, region proposals for a network were generated using selective search, CPMC, MCG, Edgeboxes etc. Faster_R-CNN is the first work to demonstrate generating region proposals using deep learning.\n",
    "\n",
    "The network contains a convolution module, on top of which there will be one regression layer, which predicts the location of the box inside the anchor\n",
    "To generate region proposals, we slide a small network over the convolutional feature map output that we obtained in the feature extraction module. This small network takes as input an n x n spatial window of the input convolutional feature map. Each sliding window is mapped to a lower-dimensional feature [512 features]. This feature is fed into two sibling fully connected layers\n",
    "A box regrression layer\n",
    "A box classification layer\n",
    "we use n=3, as noted in Faster_R-CNN paper. We can implement this Architecture using n x n convolutional layer followed by two sibiling 1 x 1 convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "mid_channels = 512\n",
    "\n",
    "in_channels = 512 # depends on the output feature map. in vgg 16 it is equal to 512\n",
    "\n",
    "n_anchor = 9 # Number of anchors at each location\n",
    "\n",
    "conv1 = nn.Conv2d(in_channels, mid_channels, 3, 1, 1) #padding 为1，可以保持特征图的分辨率\n",
    "\n",
    "reg_layer = nn.Conv2d(mid_channels, n_anchor * 4, 1, 1, 0) #padding 为0\n",
    "cls_layer = nn.Conv2d(mid_channels, n_anchor * 2, 1, 1, 0) #I will be going to use softmax here. you can equally use sigmoid if u replace 2 with 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The paper tells that they initialized these layers with zero mean and 0.01 standard deviation for weights and zeros for base. Lets do that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# conv sliding layer\n",
    "conv1.weight.data.normal_(0, 0.01)\n",
    "conv1.bias.data.zero_()\n",
    "\n",
    "#Regression layer\n",
    "reg_layer.weight.data.normal_(0, 0.01)\n",
    "reg_layer.bias.data.zero_()\n",
    "\n",
    "# classification layer\n",
    "cls_layer.weight.data.normal_(0, 0.01)\n",
    "cls_layer.bias.data.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the outputs we got in the feature extraction state should be sent to this network to predict locations of objects with repect to the anchor and the objectness score assoiciated with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 18, 50, 50]) torch.Size([1, 36, 50, 50])\n"
     ]
    }
   ],
   "source": [
    "x = conv1(out_map) # out_map is obtained in section 1\n",
    "pred_anchor_locs = reg_layer(x)\n",
    "pred_cls_scores = cls_layer(x)\n",
    "print(pred_cls_scores.shape, pred_anchor_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets reformat these a bit and make it align with our anchor targets we designed previously. We will also find the objectness scores for each anchor box, as this is used to for proposal layer which we will discuss in the next section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22500, 4])\n",
      "torch.Size([1, 50, 50, 18])\n",
      "torch.Size([1, 22500])\n",
      "torch.Size([1, 22500, 2])\n"
     ]
    }
   ],
   "source": [
    "pred_anchor_locs = pred_anchor_locs.permute(0, 2, 3, 1).contiguous().view(1, -1, 4)\n",
    "print(pred_anchor_locs.shape)\n",
    "#Out: torch.Size([1, 22500, 4])\n",
    "pred_cls_scores = pred_cls_scores.permute(0, 2, 3, 1).contiguous()\n",
    "print(pred_cls_scores.shape)\n",
    "#Out torch.Size([1, 50, 50, 18])\n",
    "objectness_score = pred_cls_scores.view(1, 50, 50, 9, 2)[:, :, :, :, 1].contiguous().view(1, -1)\n",
    "print(objectness_score.shape)\n",
    "#Out torch.Size([1, 22500])\n",
    "pred_cls_scores  = pred_cls_scores.view(1, -1, 2)\n",
    "print(pred_cls_scores.shape)\n",
    "# Out torch.size([1, 22500, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we are done with section\n",
    "pred_cls_scores and pred_anchor_locs are the output the RPN network and the losses to updates the weights\n",
    "pred_cls_scores and objectness_scores are used as inputs to the proposal layer, which generate a set of proposal which are further used by RoI network. We will see this in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating proposals to feed Fast R-CNN network\n",
    "\n",
    "The proposal function will take the following parameters\n",
    "Weather training_mode or testing mode\n",
    "nms_thresh\n",
    "n_train_pre_nms — number of bboxes before nms during training\n",
    "n_train_post_nms — number of bboxes after nms during training\n",
    "n_test_pre_nms — number of bboxes before nms during testing\n",
    "n_test_post_nms — number of bboxes after nms during testing\n",
    "min_size — minimum height of the object required to create a proposal.\n",
    "\n",
    "The Faster R_CNN says, RPN proposals highly overlap with each other. To reduced redundancy, we adopt non-maximum supression (NMS) on the proposal regions based on their cls scores. We fix the IoU threshold for NMS at 0.7, which leaves us about 2000 proposal regions per image. After an ablation study, the authors show that NMS does not harm the ultimate detection accuracy, but substantially reduces the number of proposals. After NMS, we use the top-N ranked proposal regions for detection. In the following we training Fast R-CNN using 2000 RPN proposals. During testing they evaluate only 300 proposals, they have tested this with various numbers and obtained this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "nms_thresh = 0.7\n",
    "n_train_pre_nms = 12000\n",
    "n_train_post_nms = 2000\n",
    "n_test_pre_nms = 6000\n",
    "n_test_post_nms = 300\n",
    "min_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to do the following things to generate region of interest proposals to the network.\n",
    "convert the loc predictions from the rpn network to bbox [y1, x1, y2, x2] format.\n",
    "clip the predicted boxes to the image\n",
    "Remove predicted boxes with either height or width < threshold (min_size).\n",
    "Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "Take top pre_nms_topN (e.g. 12000 while training and 300 while testing).\n",
    "Apply nms threshold > 0.7\n",
    "Take top pos_nms_topN (e.g. 2000 while training and 300 while testing)\n",
    "\n",
    "We will look at each of the stages in the remainder of this section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "(22500, 4)\n",
      "(22500,)\n"
     ]
    }
   ],
   "source": [
    "#convert anchors format from y1, x1, y2, x2 to ctr_x, ctr_y, h, w\n",
    "anc_height = anchors[:, 2] - anchors[:, 0]\n",
    "anc_width = anchors[:, 3] - anchors[:, 1]\n",
    "anc_ctr_y = anchors[:, 0] + 0.5 * anc_height\n",
    "anc_ctr_x = anchors[:, 1] + 0.5 * anc_width\n",
    "print(anc_ctr_x.shape)\n",
    "\n",
    "pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].data.numpy()\n",
    "print(pred_anchor_locs_numpy.shape)\n",
    "print(objectness_score_numpy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500, 1)\n",
      "(22500, 1)\n"
     ]
    }
   ],
   "source": [
    "pred_anchor_locs_numpy = pred_anchor_locs[0].data.numpy()\n",
    "objectness_score_numpy = objectness_score[0].data.numpy()\n",
    "dy = pred_anchor_locs_numpy[:, [0]] #每个anchor box的dy,numpy切片，如果带[]的话，可以保持维度\n",
    "dx = pred_anchor_locs_numpy[:, [1]] #每个anchor box的dx\n",
    "dh = pred_anchor_locs_numpy[:, [2]] # 每个anchor box的dh\n",
    "dw = pred_anchor_locs_numpy[:, [3]] # dw\n",
    "print(dy.shape)\n",
    "\n",
    "ctr_y = dy * anc_height[:, np.newaxis] + anc_ctr_y[:, np.newaxis]\n",
    "ctr_x = dx * anc_width[:, np.newaxis] + anc_ctr_x[:, np.newaxis]\n",
    "h = np.exp(dh) * anc_height[:, np.newaxis]\n",
    "w = np.exp(dw) * anc_width[:, np.newaxis]\n",
    "print(w.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.         0.        52.917183  98.60744 ]\n",
      " [  0.         0.        97.90541  199.06795 ]\n",
      " [  0.         0.       198.67064  375.2675  ]\n",
      " ...\n",
      " [696.2994   745.73676  800.       800.      ]\n",
      " [602.5117   700.2239   800.       800.      ]\n",
      " [429.5132   608.8695   800.       800.      ]]\n"
     ]
    }
   ],
   "source": [
    "# 用 labelled 的 anchor boxes 與 RPN 預測的 anchor boxes來計算 ROI = [y1, x1, y2, x2] \n",
    "roi = np.zeros(pred_anchor_locs_numpy.shape, dtype=anchor_locs.dtype)\n",
    "roi[:, 0::4] = ctr_y - 0.5 * h\n",
    "roi[:, 1::4] = ctr_x - 0.5 * w\n",
    "roi[:, 2::4] = ctr_y + 0.5 * h\n",
    "roi[:, 3::4] = ctr_x + 0.5 * w\n",
    "#Out:\n",
    "# [[ -36.897102,  -80.29519 ,   54.09939 ,  100.40507 ],\n",
    "#  [ -83.12463 , -165.74298 ,   98.67854 ,  188.6116  ],\n",
    "#  [-170.7821  , -378.22214 ,  196.20844 ,  349.81198 ],\n",
    "#  ...,\n",
    "#  [ 696.17816 ,  747.13306 ,  883.4582  ,  836.77747 ],\n",
    "#  [ 621.42114 ,  703.0614  ,  973.04626 ,  885.31226 ],\n",
    "#  [ 432.86267 ,  622.48926 , 1146.7059  ,  982.9209  ]]\n",
    "\n",
    "#clip the predicted boxes to the image\n",
    "img_size = (800, 800) #Image size\n",
    "roi[:, slice(0, 4, 2)] = np.clip(\n",
    "            roi[:, slice(0, 4, 2)], 0, img_size[0])\n",
    "roi[:, slice(1, 4, 2)] = np.clip(\n",
    "    roi[:, slice(1, 4, 2)], 0, img_size[1])\n",
    "print(roi)\n",
    "#Out:\n",
    "# [[  0.     ,   0.     ,  54.09939, 100.40507],\n",
    "#  [  0.     ,   0.     ,  98.67854, 188.6116 ],\n",
    "#  [  0.     ,   0.     , 196.20844, 349.81198],\n",
    "#  ...,\n",
    "#  [696.17816, 747.13306, 800.     , 800.     ],\n",
    "#  [621.42114, 703.0614 , 800.     , 800.     ],\n",
    "#  [432.86267, 622.48926, 800.     , 800.     ]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22500,)\n",
      "[  454   457     2 ...    18 22036   890]\n",
      "(12000,) (12000, 4)\n"
     ]
    }
   ],
   "source": [
    "#remove predicted boxes with either height or width < threshold\n",
    "hs = roi[:, 2] - roi[:, 0]\n",
    "ws = roi[:, 3] - roi[:, 1]\n",
    "keep = np.where((hs >= min_size) & (ws >= min_size))[0]\n",
    "roi = roi[keep, :]\n",
    "score = objectness_score_numpy[keep]\n",
    "print(score.shape)\n",
    "#Out:\n",
    "##(22500, ) all the boxes have minimum size of 16\n",
    "\n",
    "#Sort all (proposal, score) pairs by score from highest to lowest.\n",
    "order = score.ravel().argsort()[::-1]\n",
    "print(order)\n",
    "#Out:\n",
    "#[ 889,  929, 1316, ...,  462,  454,    4]\n",
    "\n",
    "#Take top pre_nms_topN (e.g. 12000 while training and 300 while testing)\n",
    "order = order[:n_train_pre_nms]\n",
    "roi = roi[order, :]\n",
    "print(order.shape, roi.shape)\n",
    "#print(roi)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply non-maximum supression threshold > 0.7 First question, What is Non-maximum supression ? It is the process in which we remove/merge extremely highly overlapping bounding boxes. If we look at the below diagram, there are lot of overlapping bounding boxes and we want a few bounding boxes which are unique and doesn’t overlap much. We keep the threshold at 0.7. threshold defines the minimum overlapping area required to merge/remove overlapping bounding boxes\n",
    "\n",
    "The sudo code for NMS works in the following way\n",
    "\n",
    "- Take all the roi boxes [roi_array]\n",
    "- Find the areas of all the boxes [roi_area]\n",
    "- Take the indexes of order the probability score in descending order [order_array]\n",
    "keep = []\n",
    "while order_array.size > 0:\n",
    "  - take the first element in order_array and append that to keep  \n",
    "  - Find the area with all other boxes\n",
    "  - Find the index of all the boxes which have high overlap with this box\n",
    "  - Remove them from order array\n",
    "  - Iterate this till we get the order_size to zero (while loop)\n",
    "- Ouput the keep variable which tells what indexes to consider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1031 (1031, 4)\n"
     ]
    }
   ],
   "source": [
    "#Take top pos_nms_topN (e.g. 2000 while training and 300 while testing)\n",
    "y1 = roi[:, 0]\n",
    "x1 = roi[:, 1]\n",
    "y2 = roi[:, 2]\n",
    "x2 = roi[:, 3]\n",
    "areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n",
    "order = order.argsort()[::-1]\n",
    "keep = []\n",
    "while order.size > 0:\n",
    "    i = order[0]\n",
    "    keep.append(i)\n",
    "    xx1 = np.maximum(x1[i], x1[order[1:]])\n",
    "    yy1 = np.maximum(y1[i], y1[order[1:]])\n",
    "    xx2 = np.minimum(x2[i], x2[order[1:]])\n",
    "    yy2 = np.minimum(y2[i], y2[order[1:]])\n",
    "    w = np.maximum(0.0, xx2 - xx1 + 1)\n",
    "    h = np.maximum(0.0, yy2 - yy1 + 1)\n",
    "    inter = w * h\n",
    "    ovr = inter / (areas[i] + areas[order[1:]] - inter)\n",
    "    inds = np.where(ovr <= nms_thresh)[0]\n",
    "    order = order[inds + 1]\n",
    "keep = keep[:n_train_post_nms] # while training/testing , use accordingly\n",
    "roi = roi[keep] # the final region proposals\n",
    "print(len(keep), roi.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final region proposals were obtained, This is used as the input to the Fast_R-CNN object which finally tries to predict the object locations (with respect to the proposed box) and class of the object (classifcation of each proposal). First we look into how to create targets for these proposals for training this network. After that we will look into how this fast r-cnn network is implemented and pass these proposals to the network to obtain the predicted outputs. Then, we will determine the losses, We will calculate both the rpn loss and fast r-cnn loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proposal targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Fast R-CNN network takes the region proposals (obtained from proposal layer in previous section), ground truth boxes and their respective labels as inputs. It will take the following parameters\n",
    "n_sample: Number of samples to sample from roi, The default value is 128.\n",
    "pos_ratio: the number of positive examples out of the n_samples. The default values is 0.25.\n",
    "pos_iou_thesh: The minimum overlap of region proposal with any groundtruth object to consider it as positive label.\n",
    "[neg_iou_threshold_lo, neg_iou_threshold_hi] : [0.0, 0.5], The overlap value bounding required to consider a region proposal as negitive [background object]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample = 128\n",
    "pos_ratio = 0.25\n",
    "pos_iou_thresh = 0.5\n",
    "neg_iou_thresh_hi = 0.5\n",
    "neg_iou_thresh_lo = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1031, 2)\n"
     ]
    }
   ],
   "source": [
    "ious = np.empty((len(roi), 2), dtype=np.float32)\n",
    "ious.fill(0)\n",
    "for num1, i in enumerate(roi):\n",
    "    ya1, xa1, ya2, xa2 = i  \n",
    "    anchor_area = (ya2 - ya1) * (xa2 - xa1)\n",
    "    for num2, j in enumerate(bbox):\n",
    "        yb1, xb1, yb2, xb2 = j\n",
    "        box_area = (yb2- yb1) * (xb2 - xb1)\n",
    "        inter_x1 = max([xb1, xa1])\n",
    "        inter_y1 = max([yb1, ya1])\n",
    "        inter_x2 = min([xb2, xa2])\n",
    "        inter_y2 = min([yb2, ya2])\n",
    "        if (inter_x1 < inter_x2) and (inter_y1 < inter_y2):\n",
    "            iter_area = (inter_y2 - inter_y1) * \\\n",
    "(inter_x2 - inter_x1)\n",
    "            iou = iter_area / (anchor_area+ \\\n",
    "box_area - iter_area)            \n",
    "        else:\n",
    "            iou = 0.\n",
    "        ious[num1, num2] = iou\n",
    "print(ious.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n",
      "[0.         0.         0.         ... 0.03046864 0.03046921 0.00844959]\n"
     ]
    }
   ],
   "source": [
    "gt_assignment = ious.argmax(axis=1)\n",
    "max_iou = ious.max(axis=1)\n",
    "print(gt_assignment)\n",
    "print(max_iou)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 6 6 ... 6 6 6]\n"
     ]
    }
   ],
   "source": [
    "gt_roi_label = labels[gt_assignment]\n",
    "print(gt_roi_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23\n",
      "[334 554 699 844 372 336 679 615 410 722 658 399 617 373 671 741 793 619\n",
      " 398 579 533 675 411]\n"
     ]
    }
   ],
   "source": [
    "# Select the foreground rois as per the pos_iou_thesh and \n",
    "# n_sample x pos_ratio (128 x 0.25 = 32) foreground samples.\n",
    "pos_roi_per_image = 32 \n",
    "pos_index = np.where(max_iou >= pos_iou_thresh)[0]\n",
    "pos_roi_per_this_image = int(min(pos_roi_per_image, pos_index.size))\n",
    "if pos_index.size > 0:\n",
    "    pos_index = np.random.choice(\n",
    "        pos_index, size=pos_roi_per_this_image, replace=False)\n",
    "print(pos_roi_per_this_image)\n",
    "print(pos_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "[ 727  814   36  316   40   59  347  461  644  541  107  858  394  555\n",
      "  925   21  567  811  976  958  424  972  903   88  156  273  566  395\n",
      "  215  101  291  631   73  207  604  842  309  258  134  350  506  725\n",
      "  342  709  889 1026  103  403  939  911  147  414  763  130  954  369\n",
      "  383  468  327  710  270  225   96  655  478  835  851  986  172  255\n",
      "  622  863  778  906  605  638  998  376    8  868  490  379  756  132\n",
      "    6    7  413  960  380  484  116  109  516  286  448  636  562  794\n",
      "   70  932  462  427  734  491  543]\n"
     ]
    }
   ],
   "source": [
    "neg_index = np.where((max_iou < neg_iou_thresh_hi) &\n",
    "                             (max_iou >= neg_iou_thresh_lo))[0]\n",
    "neg_roi_per_this_image = n_sample - pos_roi_per_this_image\n",
    "neg_roi_per_this_image = int(min(neg_roi_per_this_image,\n",
    "                                 neg_index.size))\n",
    "if  neg_index.size > 0 :\n",
    "    neg_index = np.random.choice(\n",
    "        neg_index, size=neg_roi_per_this_image, replace=False)\n",
    "print(neg_roi_per_this_image)\n",
    "print(neg_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "keep_index = np.append(pos_index, neg_index)\n",
    "gt_roi_labels = gt_roi_label[keep_index]\n",
    "gt_roi_labels[pos_roi_per_this_image:] = 0  # negative labels --> 0\n",
    "sample_roi = roi[keep_index]\n",
    "print(sample_roi.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "bbox_for_sampled_roi = bbox[gt_assignment[keep_index]]\n",
    "print(bbox_for_sampled_roi.shape)\n",
    "#Out\n",
    "#(128, 4)\n",
    "height = sample_roi[:, 2] - sample_roi[:, 0]\n",
    "width = sample_roi[:, 3] - sample_roi[:, 1]\n",
    "ctr_y = sample_roi[:, 0] + 0.5 * height\n",
    "ctr_x = sample_roi[:, 1] + 0.5 * width\n",
    "base_height = bbox_for_sampled_roi[:, 2] - bbox_for_sampled_roi[:, 0]\n",
    "base_width = bbox_for_sampled_roi[:, 3] - bbox_for_sampled_roi[:, 1]\n",
    "base_ctr_y = bbox_for_sampled_roi[:, 0] + 0.5 * base_height\n",
    "base_ctr_x = bbox_for_sampled_roi[:, 1] + 0.5 * base_width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 4)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "eps = np.finfo(height.dtype).eps\n",
    "height = np.maximum(height, eps)\n",
    "width = np.maximum(width, eps)\n",
    "dy = (base_ctr_y - ctr_y) / height\n",
    "dx = (base_ctr_x - ctr_x) / width\n",
    "dh = np.log(base_height / height)\n",
    "dw = np.log(base_width / width)\n",
    "gt_roi_locs = np.vstack((dy, dx, dh, dw)).transpose()\n",
    "print(gt_roi_locs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast R-CNN\n",
    "取出128 個 ROI samples 的 features, 用 max pooling 調成 same size, H=7, W=7 (ROI Pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 4]) torch.Size([128])\n"
     ]
    }
   ],
   "source": [
    "rois = torch.from_numpy(sample_roi).float()\n",
    "roi_indices = 0 * np.ones((len(rois),), dtype=np.int32)\n",
    "roi_indices = torch.from_numpy(roi_indices).float()\n",
    "print(rois.shape, roi_indices.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5])\n"
     ]
    }
   ],
   "source": [
    "indices_and_rois = torch.cat([roi_indices[:, None], rois], dim=1)\n",
    "xy_indices_and_rois = indices_and_rois[:, [0, 2, 1, 4, 3]]\n",
    "indices_and_rois = xy_indices_and_rois.contiguous()\n",
    "print(xy_indices_and_rois.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 512, 7, 7])\n",
      "torch.Size([128, 25088])\n"
     ]
    }
   ],
   "source": [
    "size = (7, 7)\n",
    "adaptive_max_pool = nn.AdaptiveMaxPool2d(size[0], size[1])\n",
    "output = []\n",
    "rois = indices_and_rois.data.float()\n",
    "rois[:, 1:].mul_(1/16.0) # Subsampling ratio\n",
    "rois = rois.long()\n",
    "num_rois = rois.size(0)\n",
    "for i in range(num_rois):\n",
    "    roi = rois[i]\n",
    "    im_idx = roi[0]\n",
    "    im = out_map.narrow(0, im_idx, 1)[..., roi[2]:(roi[4]+1), roi[1]:(roi[3]+1)]\n",
    "    tmp = adaptive_max_pool(im)\n",
    "    output.append(tmp[0])\n",
    "output = torch.cat(output, 0)\n",
    "print(output.size())\n",
    "#Out:\n",
    "# torch.Size([128, 512, 7, 7])\n",
    "# Reshape the tensor so that we can pass it through the feed forward layer.\n",
    "k = output.view(output.size(0), -1)\n",
    "print(k.shape)\n",
    "#Out:\n",
    "# torch.Size([128, 25088])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "128 個 ROI samples 的 boxes + features (7x7x512) 送到 Detection network 預測輸入影像的物件 bounding box 與 class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "roi_head_classifier = nn.Sequential(*[nn.Linear(25088, 4096),\n",
    "                                      nn.Linear(4096, 4096)])\n",
    "cls_loc = nn.Linear(4096, 21 * 4) # (VOC 20 classes + 1 background. Each will have 4 co-ordinates)\n",
    "cls_loc.weight.data.normal_(0, 0.01)\n",
    "cls_loc.bias.data.zero_()\n",
    "score = nn.Linear(4096, 21) # (VOC 20 classes + 1 background)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 84]) torch.Size([128, 21])\n"
     ]
    }
   ],
   "source": [
    "k = roi_head_classifier(k)\n",
    "roi_cls_loc = cls_loc(k)\n",
    "roi_cls_score = score(k)\n",
    "print(roi_cls_loc.shape, roi_cls_score.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# section 7 Loss functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RPN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 22500, 4])\n",
      "torch.Size([1, 22500, 2])\n",
      "(22500, 4)\n",
      "(22500,)\n"
     ]
    }
   ],
   "source": [
    "print(pred_anchor_locs.shape)\n",
    "print(pred_cls_scores.shape)\n",
    "print(anchor_locations.shape)\n",
    "print(anchor_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22500, 4]) torch.Size([22500, 2]) torch.Size([22500, 4]) torch.Size([22500])\n"
     ]
    }
   ],
   "source": [
    "rpn_loc = pred_anchor_locs[0]\n",
    "rpn_score = pred_cls_scores[0]\n",
    "gt_rpn_loc = torch.from_numpy(anchor_locations)\n",
    "gt_rpn_score = torch.from_numpy(anchor_labels)\n",
    "print(rpn_loc.shape, rpn_score.shape, gt_rpn_loc.shape, gt_rpn_score.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.6931, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "rpn_cls_loss = F.cross_entropy(rpn_score, gt_rpn_score.long(), ignore_index = -1)\n",
    "print(rpn_cls_loss)\n",
    "#Out:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22500, 4])\n"
     ]
    }
   ],
   "source": [
    "pos = gt_rpn_score > 0\n",
    "mask = pos.unsqueeze(1).expand_as(rpn_loc)\n",
    "print(mask.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([18, 4]) torch.Size([18, 4])\n"
     ]
    }
   ],
   "source": [
    "mask_loc_preds = rpn_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_rpn_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.2075, grad_fn=<SumBackward0>)\n"
     ]
    }
   ],
   "source": [
    "x = torch.abs(mask_loc_targets - mask_loc_preds)\n",
    "rpn_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\n",
    "print(rpn_loc_loss.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.3640, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "rpn_lambda = 10.\n",
    "N_reg = (gt_rpn_score >0).float().sum()\n",
    "rpn_loc_loss = rpn_loc_loss.sum() / N_reg\n",
    "rpn_loss = rpn_cls_loss + (rpn_lambda * rpn_loc_loss)\n",
    "print(rpn_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Fast R-CNN loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 84])\n",
      "torch.Size([128, 21])\n",
      "(128, 4)\n",
      "(128,)\n",
      "torch.Size([128, 4]) torch.Size([128])\n",
      "torch.Size([])\n",
      "torch.Size([128, 21, 4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([128, 4])\n",
      "torch.Size([23, 4]) torch.Size([23, 4])\n",
      "tensor(2.1932, grad_fn=<SumBackward0>)\n",
      "tensor([[3.1561, 3.1140, 3.5262, 3.4983],\n",
      "        [3.0734, 3.3998, 3.0811, 3.7409],\n",
      "        [3.0733, 3.1111, 3.1484, 3.0569],\n",
      "        [3.3515, 3.0767, 3.0812, 3.0555],\n",
      "        [3.0870, 3.0557, 3.5262, 3.4983],\n",
      "        [3.0640, 3.1140, 3.5262, 3.4983],\n",
      "        [3.0547, 3.1509, 3.0546, 3.5456],\n",
      "        [3.1333, 3.3244, 3.4025, 3.0569],\n",
      "        [3.0563, 3.1523, 3.5262, 3.4983],\n",
      "        [3.2778, 3.0778, 3.5784, 3.0551],\n",
      "        [3.0963, 3.2703, 3.1561, 3.4225],\n",
      "        [3.1612, 3.1187, 5.2256, 3.0845],\n",
      "        [3.0877, 3.4312, 3.2006, 3.5556],\n",
      "        [3.1375, 3.0548, 3.4378, 3.4151],\n",
      "        [3.3500, 3.1036, 3.4425, 3.0635],\n",
      "        [3.0877, 3.0551, 3.2006, 3.5556],\n",
      "        [3.0575, 3.1337, 3.0686, 3.5700],\n",
      "        [3.1911, 3.3224, 3.3772, 3.0572],\n",
      "        [3.1090, 3.1036, 5.1523, 3.0707],\n",
      "        [3.0546, 3.4906, 3.0571, 3.0569],\n",
      "        [3.0551, 4.0897, 3.0546, 3.5346],\n",
      "        [3.2197, 3.1156, 3.7207, 3.4857],\n",
      "        [3.2365, 3.1405, 3.4378, 3.4151]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#predicted\n",
    "print(roi_cls_loc.shape)\n",
    "print(roi_cls_score.shape)\n",
    "\n",
    "#actual\n",
    "print(gt_roi_locs.shape)\n",
    "print(gt_roi_labels.shape)\n",
    "\n",
    "#Converting ground truth to torch variable\n",
    "gt_roi_loc = torch.from_numpy(gt_roi_locs)\n",
    "gt_roi_label = torch.from_numpy(np.float32(gt_roi_labels)).long()\n",
    "print(gt_roi_loc.shape, gt_roi_label.shape)\n",
    "\n",
    "#classification loss\n",
    "roi_cls_loss = F.cross_entropy(roi_cls_score, gt_roi_label, ignore_index=-1)\n",
    "print(roi_cls_loss.shape)\n",
    "\n",
    "#Regression loss \n",
    "n_sample = roi_cls_loc.shape[0]\n",
    "roi_loc = roi_cls_loc.view(n_sample, -1, 4)\n",
    "print(roi_loc.shape)\n",
    "#Out:\n",
    "#torch.Size([128, 21, 4])\n",
    "roi_loc = roi_loc[torch.arange(0, n_sample).long(), gt_roi_label]\n",
    "print(roi_loc.shape)\n",
    "\n",
    "# For Regression we use smooth L1 loss as defined in the Fast RCNN paper\n",
    "pos = gt_roi_label > 0\n",
    "mask = pos.unsqueeze(1).expand_as(roi_loc)\n",
    "print(mask.shape)\n",
    "\n",
    "# take those bounding boxes which have positve labels\n",
    "mask_loc_preds = roi_loc[mask].view(-1, 4)\n",
    "mask_loc_targets = gt_roi_loc[mask].view(-1, 4)\n",
    "print(mask_loc_preds.shape, mask_loc_targets.shape)\n",
    "\n",
    "x = torch.abs(mask_loc_targets.cpu() - mask_loc_preds.cpu())\n",
    "roi_loc_loss = ((x < 1).float() * 0.5 * x**2) + ((x >= 1).float() * (x-0.5))\n",
    "print(roi_loc_loss.sum())\n",
    "\n",
    "#total roi loss\n",
    "roi_lambda = 10.\n",
    "roi_loss = roi_cls_loss + (roi_lambda * roi_loc_loss)\n",
    "print(roi_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total loss\n",
    "\n",
    "Now we need to combine the RPN loss and Fast-RCNN loss to compute the total loss for 1 iteration. this is a simple addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.5201, 4.4779, 4.8902, 4.8623],\n",
      "        [4.4373, 4.7637, 4.4451, 5.1049],\n",
      "        [4.4372, 4.4750, 4.5124, 4.4208],\n",
      "        [4.7155, 4.4407, 4.4451, 4.4195],\n",
      "        [4.4510, 4.4197, 4.8902, 4.8623],\n",
      "        [4.4280, 4.4779, 4.8902, 4.8623],\n",
      "        [4.4187, 4.5149, 4.4186, 4.9096],\n",
      "        [4.4973, 4.6883, 4.7665, 4.4208],\n",
      "        [4.4202, 4.5163, 4.8902, 4.8623],\n",
      "        [4.6417, 4.4418, 4.9423, 4.4191],\n",
      "        [4.4603, 4.6343, 4.5201, 4.7864],\n",
      "        [4.5251, 4.4827, 6.5896, 4.4484],\n",
      "        [4.4517, 4.7952, 4.5646, 4.9195],\n",
      "        [4.5014, 4.4187, 4.8017, 4.7790],\n",
      "        [4.7140, 4.4676, 4.8065, 4.4275],\n",
      "        [4.4517, 4.4191, 4.5646, 4.9195],\n",
      "        [4.4215, 4.4977, 4.4326, 4.9340],\n",
      "        [4.5551, 4.6864, 4.7412, 4.4212],\n",
      "        [4.4730, 4.4676, 6.5163, 4.4346],\n",
      "        [4.4186, 4.8545, 4.4211, 4.4208],\n",
      "        [4.4191, 5.4536, 4.4186, 4.8986],\n",
      "        [4.5836, 4.4795, 5.0846, 4.8497],\n",
      "        [4.6005, 4.5045, 4.8017, 4.7790]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "total_loss = rpn_loss + roi_loss\n",
    "print(total_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
